##### 시험 요약 공부

### Overfitting
    과적합 (train 에 대해서는 잘 하지만 Test 에 대해서는 못하는 일반성이 떨어지는 것)
    원인은 데이터셋이 너무 적거나, 모델이 너무 복잡한 경우


### 앙상블 학습의 대표적인 NonLinear model (비선형 모델)
#### Random Forest 
     sample 도 feature 도 랜덤하게 뽑아서 트리를 만들고 .. 
    (데이터 샘플과 특성을 일부씩 랜덤으로 뽑고, 결정 트리 모델을 여러개 사용하는 것)

    랜덤 포레스트 란 여러개의 Decision Trees (결정 트리) 를 사용하는 앙상블 학습 기법이다.
    여러개의 Tree 를 독립적으로 학습시키고, 각 트리의 예측 결과를 투표 또는 평균내어 최종 예측을 만드는 것을 아이디어로 하고 있다.
    랜덤 포레스트는 분류와 회귀 문제 모두에 쓰인다.
    
    랜덤 포레스트 특징:
    부트스트랩 샘플링 - 각 트리를 학습 할 때, 전체 데이터에서 중복 허용 샘플링을 통해, 데이터의 일부분만 사용한다.
    랜덤성 - 트리 분할 시 무작위로 feature(특징)를 선택하고, 그 중 최적의 특징을 선택해 트리를 구성하여, Tree 간의 상관성을 낮춘다.
    다양성 - 여러 트리의 예측 결과를 조합하므로, Overfitting 에 대한 저항력이 강하다.
    병렬화 가능 - 각 Tree 가 독립적으로 학습되므로, 병렬 처리가 가능하다.

    예를 들면, 
    분류 문제에서는 가장 많은 트리가 예측한 class 를 최종 예측 값으로 선택하거나,
    회귀 문제에서는 트리들의 예측값 평균을 내어 최종 결과로 사용하는 것이다. 

#### Gradient Boosting (그래디언트 B) 
    그래디언트 부스팅이란, 순차적으로 약한 학습기 (일반적으로 결정 트리)를 학습시켜 나가는 앙상블 기법을 말한다.
    첫번째 학습기가 만든 예측 오류(잔차, residuals)를 다음 학습기가 수정해나가며,
    이전 단계에서 발생한 오류를 최소화 하는 방향으로 학습하여, 학습 할 수록 오류가 줄어드는
    구조가 기본 아이디어이다.

    그래디언트 부스팅 특징:
    순차적 학습 
    오류 수정 - 이전 학습기가 예측하지 못한 오류(잔차)를 학습하여 성능을 개선한다.
    과적합 가능성 - 과적합의 위험이 있다. 이 때문에 learning rate(학습률)과 Tree의 depth 등 하이퍼파라미터 조정이 중요하다.
    효율성 - 순차 학습으로, 병렬 처리가 어려울 수 없지만 트리의 개수, 깊이를 잘 조정하면 효율적인 예측 모델을 만들 수 있다.


    >  Q. 거의 경사하강법과 비슷하지 않나..? 
    >  A. 경사하강법은 '최적화 알고리즘', 딥러닝과 선형 회귀에서 weight(모델의 가중치)를 업데이트하며 손실 함수를 최소화 하는 방향으로 이동하는 방식
    >  A. Gradient Boosting 은 앙상블 학습 기법, 결정 트리와 결합해 사용하며 각 트리 학습 시 경사하강법의 아이디어를 사용한 방식.
    


### 분류 모델에서 주로 사용하는 손실 함수
#### Cross Entropy

### F1-score

$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

#### 정밀도(Precision):
$Precision = \frac{TP}{TP + FP}$

#### 재현율(Recall):
$Recall = \frac{TP}{TP + FN}$


### Mini-batch 기법
    데이터가 엄청 크면 시간이 너무 많이 걸린다. (딥러닝)
    그래서 한번 할 때 마다 전체 데이터를 쓰지 않고, 조각조각 나눠서 쓴다. (batch)

    이 때 미니배치 경사하강법을 사용한다.
    Mini-batch Gradient Descent:
    전체 데이터를 작은 batch 로 나눠서, '각각의' 배치에서 기울기를 계산하고 가중치를 업데이트 하는 방식
    여러번의 작은 업데이트를 통해 손실을 빠르게 줄일 수 있다는 장점이 있다!
    

#### Stocastic Gradient Descent (SGD, 확률적 경사 하강법)
    하나의 샘플만을 사용해서 기울기를 계산하고 가중치를 업데이트하는 방법.
    학습 속도가 빠르다는 장점이 있으나, 최적의 최소값에 도달하지 못하고 진동 할 가능성이 있다.

    stocastic Gradient Descent (Batch 임) -> 모든 batch 에 대해 1번 학습을 완료하는 것이 1 epoch

#### 모델을 최적화 (optimize) 한다는 것은?

    Loss 가 최소가 되는 지점을 찾아가는 것,
    모델의 최적화를 위해 손실 함수를 미분값을 이용해 반대방향을 이용해서 옮겨가며 최저점을 찾는 방법 : 경사하강법

##### K Fold Cross Validation 이란? (k-Fold 교차 검증)
    vali 데이터가 충분하지 않을 때에는 block 을 나누고 돌아가면서 훈련시키고 검증을 한다.
    
    k-Fold 교차 검증은 머신러닝에서 모델의 성능을 평가하기 위한 교차 검증 방법 중 하나이다.
    
    데이터셋을 여러개의 Fold (부분 집합) 으로 나누어,
    각각의 Fold 가 한 번씩 test set 역할을 하도록 하여, 모델을 학습하고 평가하는 방법이다.
    이 검증 방식은, 모델이 Overfitting 되지 않도록 돕는다.

    -> k 개의 Fold 중 하나의 Fold 를 vali set 으로, 나머지 k-1 개를 train set 으로
    -> 이 과정을 k 번 반복

    Q. 그럼 이게 Fold 가 batch 라는건가..?
    A. Fold: Fold 는 전체 데이터셋을 k개로 나눈 각각의 부분을 의미한다.
        예를 들어, k=5 이면, 하나의 Fold 는 전체 데이터셋의 1/5 인 것이다.
        각 Fold 는 순차적으로 test set 으로 사용되며, 나머지 Fold 는 train set 으로 사용된다.
    A. Batch: Batch는 학습 시 한 번에 모델로 전달되는 데이터의 그룹을 의미한다. 
        Batch는 메모리의 용량과 계산 효율성에 기반하여 선택되는 것이다.

    즉 Fold 는 학습-검증-학습-검증... 단계에서 과적합을 피하기 위해 사용.
    Batch 는 모델을 훈련시킬 하드웨어의 스펙에 맞추기 위해 잘게 나누어서 사용.  용도와 목적이 다름.

##### Cross Val Score 란?
    Cross Val Score() 라는 메소드가 있다.
    이것은 k-Fold 교차 검증을 수행하는 기능을 제공한다.

    각 폴드에서 모델을 훈련하고, 평가 한 후, 해당 평가 점수를 반환한다. 
```python
cross_val_score(estimator, X, y=None, *, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=np.nan)
# estimator: 훈련시킬 모델 객체 (RandomForestClassifier, SVC 등)
# X: 입력 데이터, 독립 변수 (feature data set 또는 DataFrame 등)
# y: 타겟 데이터, 종속 변수 (label, 1차원 배열 등)
# scoreing: 모델의 평가 기준을 설정하는 매개 변수, 기본값 None (정확도 accuracy, 회귀에서는 R^2 score 등)
# cv: k값, 교차 검증에서 사용할 Fold의 개수, 기본값 5 
# n_jobs: 병렬 처리에 사용할 CPU 코어 수 (-1로 사용 시 모든 CPU 코어 사용, 병렬 처리)
# verbose: 출력 메시지의 상세도를 조절하는 매개변수, 값이 클수록 많은 정보를 출력한다.
# error_score: 에러 발생 시 반환할 점수를 지정. 기본값 np.nan (NaN)

# iris data 사용 예시
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# 데이터 로드
iris = load_iris()
X = iris.data
y = iris.target

# 모델 생성
model = RandomForestClassifier()

# 교차 검증 수행 (정확도 기준으로 5-Fold 교차 검증)
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

# 각 폴드별 점수 출력
print(f'Fold별 정확도: {scores}')

# 평균 정확도 출력
print(f'평균 정확도: {scores.mean()}')

```

#### Standard Scale (표준정규분포화)
    표준 정규 분포화 란, 데이터 전처리 과정에서 사용되는 Standardization 방법을 의미한다.
    데이터의 각 특성 (변수)을 평균이 0, 분산이 1인 정규 분포로 변환한다.
    (어떤 값에 대해 평균을 뺀 다음 sigma 로 나누고 ..)
    
    standard Scaling 을 위해 사용하는 scikit-learn 의 클래스가 StandardScalar 이다.
```python
# StandardScalar 사용 예시

from sklearn.preprocessing import StandardScaler

# 데이터 준비
X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

# StandardScaler 객체 생성
scaler = StandardScaler()

# 학습 (평균과 표준 편차 계산)
scaler.fit(X)

# 데이터 변환 (표준화)
X_scaled = scaler.transform(X) # 이 X_scaled 에 표준정규화계산된 데이터가 들어간다.

print("표준화된 데이터:", X_scaled)

```
    
#### 쿠퍼스 - 말뭉치

#### KNN 알고리즘

#### Decesion Tree 모델


#### 서포트 벡터 머신과 Linear 모델의 차이점 알기

##### TODO 아래는 마저 정리해야함...
```


KNN 알고리즘 -> 
비선형 모델 중 무언가를 prediction 할 때 classification 을 하고 싶을때,
어떤 분류 안에 들어있는 개수
주어진 sample 의 특성 공간에서 거리가 가장 가까운 k 개를 보고, 그것의 평균 치를 보고
샘플이 속할 class 를 예측하는 알고리즘 



Decesion Tree 모델 ->  

데이터가 모여 있을 때, 어떤 feature 를 기준으로 두개를 나누는데, 그 기준이 되는 것이
현재 상태 복잡도, 불순도 (gini) 를 계산 후, weigthed gini 가 얼마나 줄어들 건지를 보고..
불순도가 가장 낮아지는 방향으로 바꾼다..



gini 수식, 계산 방법 알기 => gini = 1 - P^2...



Loss 가 점점 줄어야 하는데 다시 올라가는 경우..

그 적절한 지점을 찾아야 하는 데

Tree 의 Max Depth 를 지정해주거나..





```

