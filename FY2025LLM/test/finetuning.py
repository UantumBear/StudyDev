""" Llama 3.2 1B íŒŒì¸íŠœë‹
# python FY2025LLM/test/finetuning.py
# í™˜ê²½
@OS Windows
@Python 3.12.1
@NVIDIA-SMI 560.94 / Driver Version: 560.94 / CUDA Version: 12.6
@torch 2.5.1+cu121
@trl 0.18.1
@transformers 4.52.4
@dataset 3.6.0
"""
# from torch.testing._internal.common_nn import padding_mode PyTorchì˜ nn.Conv2d ë“±ì—ì„œ ì“°ì´ëŠ” ì˜µì…˜ ì¤‘ í•˜ë‚˜ë¡œ, LLM ì—ì„œëŠ” ë¶ˆí•„ìš”í•˜ë‹¤ê³  í•¨.
from transformers import default_data_collator
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from langdetect import detect
# LlamaTokenizerFastë¥¼ ëª…ì‹œì ìœ¼ë¡œ import í•œë‹¤. 'tokenizer.model' íŒŒì¼ì´ ì˜¬ë°”ë¥´ê²Œ ì €ì¥ë˜ë„ë¡ ë³´ì¥í•˜ê¸° ìœ„í•¨ì´ë‹¤.
from transformers import LlamaTokenizerFast
from datasets import load_dataset
from transformers import EarlyStoppingCallback # í•™ìŠµ ì¡°ê¸°ì¢…ë£Œ
import torch
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
from trl import SFTTrainer, SFTConfig
import json
import os
from peft import get_peft_model, TaskType, LoraConfig, PeftModel

# ê²½ë¡œ ì„¤ì •
MODEL_PATH = "FY2025LLM/models/llama3.2-1B-hf"
DATASET_PATH = "FY2025LLM/data/converted/CarrotAI/cleaned_llama3_dataset.jsonl"
SAVE_PATH = "FY2025LLM/models/llama3.2-1B-hf/finetuned/model_v1"
os.makedirs(SAVE_PATH, exist_ok=True)

# GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸, í™˜ê²½ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì„±ëŠ¥ í™•ì¸
print(torch.cuda.get_device_name(0))
print(f"Allocated: {round(torch.cuda.memory_allocated(0)/1024**3, 1)} GB")
print(f"Cached:    {round(torch.cuda.memory_reserved(0)/1024**3, 1)} GB")

""" Step 1. í† í¬ë‚˜ì´ì € ì…‹íŒ… """
# tokenizer & model ë¡œë“œ : ë¡œì»¬ ë””ë ‰í† ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
# í† í¬ë‚˜ì´ì € ë¡œë“œ: AutoTokenizer ëŒ€ì‹  LlamaTokenizerFastë¥¼ ì‚¬ìš©
tokenizer = LlamaTokenizerFast.from_pretrained(
    MODEL_PATH,
    use_fast=True,
    local_files_only=True
)
tokenizer.pad_token = tokenizer.eos_token # íŒ¨ë”© ì„¤ì •
tokenizer.padding_side = "right" # ì˜¤ë¥¸ìª½ìœ¼ë¡œ íŒ¨ë”©ì„ ë¶™ì„, Llama, GPT ê³„ì—´

# special_tokens_dict = {
#     "pad_token": tokenizer.eos_token  # ë˜ëŠ” ì›í•˜ëŠ” í† í°
# }
# tokenizer.add_special_tokens(special_tokens_dict)


# ê³µì‹ LLaMA 3.2ìš© chat template ìˆ˜ë™ ì‚½ì…
tokenizer.chat_template = (
    "{% for message in messages %}"
    "{% if message['role'] == 'system' %}<|start_header_id|>system<|end_header_id|>\n{{ message['content'] }}\n"
    "{% elif message['role'] == 'user' %}<|start_header_id|>user<|end_header_id|>\n{{ message['content'] }}\n"
    "{% elif message['role'] == 'assistant' %}<|start_header_id|>assistant<|end_header_id|>\n{{ message['content'] }}\n"
    "{% endif %}{% endfor %}"
)
# print("ì¶”ê°€ëœ special token IDë“¤:", tokenizer.added_tokens_encoder.keys())
# print("ë‚´ìš©:", tokenizer.added_tokens_encoder)
# set CUDA_LAUNCH_BLOCKING=1 ì„ í„°ë¯¸ë„ì—ì„œ ì„¤ì •í•˜ê²Œ ë””ë²„ê¹…

""" ================================================ ë°ì´í„° ì „ì²˜ë¦¬ =============================================="""
# 1. ë°ì´í„°ì…‹ ë¡œë“œ (JSONL)
dataset = load_dataset("json", data_files=DATASET_PATH)
# 2. í† í° ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë„ˆë¬´ í° ë°ì´í„°ëŠ” í•„í„°ë§
MAX_LENGTH = 1024
def check_token_length(example):
    formatted_text = tokenizer.apply_chat_template( # ëŒ€í™” ë‚´ìš©ì„ í…œí”Œë¦¿ì— ì ìš©í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“ ë‹¤.
        example['messages'],
        tokenize=False,
        add_generation_prompt=False
    )
    return len(tokenizer.encode(formatted_text)) <= MAX_LENGTH # í† í¬ë‚˜ì´ì§• í›„ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•œë‹¤. (MAX_LENGTHë³´ë‹¤ ê¸´ ë°ì´í„°ë¥¼ ì œê±°)
print(f"Original dataset size: {len(dataset['train'])}")
filtered_dataset = dataset.filter(check_token_length)
print(f"Filtered dataset size (<= {MAX_LENGTH} tokens): {len(filtered_dataset['train'])}")
# 3. ì†ŒìŠ¤ êµ¬ë™ì„ ìœ„í•´ (ì‹¤ì œ íŒŒì¸íŠœë‹ì´ ì•„ë‹Œ, íŒŒì¸íŠœë‹ ì†ŒìŠ¤ ì²´í¬ ìš©ë„) ë°ì´í„°ì˜ ì–‘ì„ ëœë¤ìœ¼ë¡œ ì¶•ì†Œì‹œí‚¨ë‹¤.
SAMPLE_SIZE = 500
# í•„í„°ë§ëœ ë°ì´í„°ì…‹ì„ ì„ì€ í›„, ì•ì—ì„œë¶€í„° SAMPLE_SIZE ë§Œí¼ ì„ íƒ, ë§Œì•½ í•„í„°ë§ëœ ë°ì´í„°ê°€ SAMPLE_SIZEë³´ë‹¤ ì ìœ¼ë©´, ê°€ëŠ¥í•œ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©.
num_available_samples = len(filtered_dataset["train"])
if num_available_samples < SAMPLE_SIZE:
    print(f"Warning: Available samples ({num_available_samples}) is less than the requested sample size ({SAMPLE_SIZE}). Using all available samples.")
    sample_size_to_use = num_available_samples
else:
    sample_size_to_use = SAMPLE_SIZE

final_dataset_for_split = filtered_dataset["train"].shuffle(seed=42).select(range(sample_size_to_use))
print(f"Sampled dataset size for training/evaluation: {len(final_dataset_for_split)}")


# ìƒ˜í”Œë§ëœ ë°ì´í„°ì…‹ì„ í•™ìŠµ/ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• 
split_dataset = final_dataset_for_split.train_test_split(test_size=0.2, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]


# ë°ì´í„° ì „ì²˜ë¦¬
# SFTTrainerê°€ ë‚´ë¶€ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ `formatting_func`ë¥¼ ì •ì˜, (trl ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê¶Œì¥í•˜ëŠ” ë°©ë²•)
def formatting_prompts_func(example):
    # example['messages']ëŠ” [{'role': 'user', ...}, {'role': 'assistant', ...}] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
    # apply_chat_templateì„ ì‚¬ìš©í•˜ë©´ SFTTrainerê°€ ë‚´ë¶€ì ìœ¼ë¡œ prompt ë¶€ë¶„ì˜ lossë¥¼ ë¬´ì‹œí•˜ë„ë¡ ì²˜ë¦¬í•œë‹¤.
    return [tokenizer.apply_chat_template(conversation, tokenize=False) for conversation in example["messages"]]
    # return tokenizer.apply_chat_template(example['messages'], tokenize=False, add_generation_prompt=False)


# ëª¨ë¸ ë¡œë“œ
# bf16 ì§€ì› ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì¸ë‹¤.
is_bf16_supported = torch.cuda.is_available() and torch.cuda.is_bf16_supported()
if not is_bf16_supported:
    print("\nWarning: Your GPU does not support bf16. Falling back to fp16, which can be unstable.\n")
else:
    print(f"is_bf16_supported : {is_bf16_supported}")

""" Trainer ì„¤ì • """
training_args = TrainingArguments(
    output_dir=SAVE_PATH,
    num_train_epochs=1,
    per_device_train_batch_size=1,
    # ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì (accumulation)ì„ ì‚¬ìš©í•œë‹¤.
    # ì‹¤ì§ˆì ì¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” batch_size * accumulation_steps = 1 * 4 = 4ê°€ ëœë‹¤.
    gradient_accumulation_steps=1,
    # bf16ì€ ë” ë„“ì€ ë™ì  ë²”ìœ„ë¥¼ ê°€ì ¸ fp16ë³´ë‹¤ ìˆ˜ì¹˜ì ìœ¼ë¡œ í›¨ì”¬ ì•ˆì •ì ì´ë¼ê³  í•œë‹¤.
    bf16=is_bf16_supported, # (Ampere ì•„í‚¤í…ì²˜ GPU(ì˜ˆ: RTX 30xx, A100) ì´ìƒì—ì„œ ì§€ì›)
    fp16=not is_bf16_supported, # bf16ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ fp16ì„ ì‚¬ìš©í•œë‹¤. (ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„± ìˆìŒ)
    learning_rate=1e-5,
    logging_steps=100,
    save_steps=1000,
    eval_strategy="steps", # í•™ìŠµ ë„ì¤‘ ì£¼ê¸°ì ìœ¼ë¡œ í‰ê°€(evaluation)ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ì •
    eval_steps=1000,
    save_total_limit=1,
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
    # optim="paged_adamw_32bit", # For linux
    lr_scheduler_type="constant",
    report_to="tensorboard"
)
peft_params = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
)
# quant_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.float16,
#     bnb_4bit_use_double_quant=False,
# )

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    local_files_only=True,
    # quantization_config=quant_config, Linux ìš© ì˜µì…˜
    # device_map="auto" # vRAM ë¶„ì‚° ë° ëª¨ë¸ ë¶„í•  ë¡œë”©ì— ìœ ë¦¬í•˜ë‹¤ê³  í•œë‹¤. TODO ê³µë¶€í•˜ê¸°
    # accelerateë¥¼ ì‚¬ìš©í•´ì„œ ìë™ìœ¼ë¡œ GPUì™€ CPUë¥¼ ë‚˜ëˆ ì„œ ë¡œë”© ... Trainerì™€ ì¶©ëŒí•˜ì—¬ ì£¼ì„
    # ëª¨ë¸ì„ 32ë¡œ ë¶€ë¥´ê³ , Trainer ì—ì„œ 16ìœ¼ë¡œ ë³€í™˜í•˜ë ¤ê³  í•˜ë©´ CUDA Memory out ì´ ë‚œë‹¤.
    torch_dtype=torch.bfloat16 if is_bf16_supported else torch.float16,
    attn_implementation="sdpa" # Flash Attention ì€ Linux í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥
)
# LoRA ì ìš©
model = get_peft_model(model, peft_params) # peft ë¡œ í•˜ì 6ì‹œê°„30ë¶„ -> 1ì‹œê°„ 30ë¶„ ì •ë„ë¡œ ì¤„ì–´ë“¤ì—ˆë‹¤.
model.print_trainable_parameters()  # í™•ì¸ìš©

# model.resize_token_embeddings(len(tokenizer))
# ëª¨ë¸ì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„° dtype ì¶œë ¥ (float í™•ì¸ìš©ë„) ... Loss nan ì´ ë°œìƒí•˜ì—¬ í™•ì¸
print(next(model.parameters()).dtype) # ê¸°ë³¸: torch.float32
model.to(device)  # ì§ì ‘ GPUë¡œ ì´ë™



token_lengths = []  # ë¶„ì„ìš©

# í•™ìŠµ ì „ í™•ì¸
print(f"Model dtype: {model.dtype}")
print(f"Model device: {next(model.parameters()).device}")

# í•™ìŠµ ì‹œì‘
print("Start training...")
# # SFTTrainer ì •ì˜
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì›ë³¸ ë°ì´í„°ì…‹ì„ ì „ë‹¬
    eval_dataset=eval_dataset,  # ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì›ë³¸ ë°ì´í„°ì…‹ì„ ì „ë‹¬
    formatting_func=formatting_prompts_func,  # ìœ„ì—ì„œ ì •ì˜í•œ í¬ë§¤íŒ… í•¨ìˆ˜ ì§€ì •
    processing_class=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] # ê²€ì¦ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•ŠëŠ” epochê°€ 2ë²ˆ ì—°ì† ë°œìƒí•˜ë©´ í•™ìŠµì„ ì¤‘ë‹¨.
)


# Trainer í•™ìŠµ íŒŒë¼ë¯¸í„° 1. FullFineTuning Sets
# training_args_FullFineTuning = TrainingArguments(
#     output_dir=SAVE_PATH,
#     per_device_train_batch_size=1,
#     # ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ê·¸ë˜ë””ì–¸íŠ¸ ì¶•ì (accumulation)ì„ ì‚¬ìš©í•œë‹¤.
#     # ì‹¤ì§ˆì ì¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” batch_size * accumulation_steps = 1 * 4 = 4ê°€ ëœë‹¤.
#     gradient_accumulation_steps=4,
#     #
#     # bf16ì€ ë” ë„“ì€ ë™ì  ë²”ìœ„ë¥¼ ê°€ì ¸ fp16ë³´ë‹¤ ìˆ˜ì¹˜ì ìœ¼ë¡œ í›¨ì”¬ ì•ˆì •ì ì´ë¼ê³  í•œë‹¤.
#     bf16=is_bf16_supported, # (Ampere ì•„í‚¤í…ì²˜ GPU(ì˜ˆ: RTX 30xx, A100) ì´ìƒì—ì„œ ì§€ì›)
#     fp16=not is_bf16_supported, # bf16ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ fp16ì„ ì‚¬ìš©í•œë‹¤. (ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥ì„± ìˆìŒ)
#     num_train_epochs=1,
#     learning_rate=5e-6,
#     logging_steps=10,
#     save_steps=100,
#     eval_strategy="steps", # í•™ìŠµ ë„ì¤‘ ì£¼ê¸°ì ìœ¼ë¡œ í‰ê°€(evaluation)ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ì •
#     eval_steps=100,
#     save_total_limit=1,
#     load_best_model_at_end=True,
#     metric_for_best_model="loss",
#     greater_is_better=False,
#     report_to="none"
# )
# # SFTTrainer ì •ì˜
# trainer_FullFineTuning = SFTTrainer(
#     model=model,
#     args=training_args_FullFineTuning,
#     train_dataset=train_dataset,  # ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì›ë³¸ ë°ì´í„°ì…‹ì„ ì „ë‹¬
#     eval_dataset=eval_dataset,  # ì „ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì›ë³¸ ë°ì´í„°ì…‹ì„ ì „ë‹¬
#     formatting_func=formatting_prompts_func,  # ìœ„ì—ì„œ ì •ì˜í•œ í¬ë§¤íŒ… í•¨ìˆ˜ ì§€ì •
#     processing_class=tokenizer,
#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] # ê²€ì¦ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•ŠëŠ” epochê°€ 2ë²ˆ ì—°ì† ë°œìƒí•˜ë©´ í•™ìŠµì„ ì¤‘ë‹¨.
# )
"""  Full FineTuning END """



# í•™ìŠµ
trainer.train()




# ëª¨ë¸ ì €ì¥
print("===== ë‚´ë¶€ backend í™•ì¸ =====")#
print("Tokenizer class:", tokenizer.__class__)
print("Has backend_tokenizer:", hasattr(tokenizer, "backend_tokenizer"))
print("Contains tokenizer.model?", os.path.exists(os.path.join(SAVE_PATH, "tokenizer.model")))
print("===== ëª¨ë¸ ì €ì¥ ì§ì „ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ì •ë³´ í™•ì¸ =====")
print("ğŸ“Œ tokenizer vocab size:", tokenizer.vocab_size) # ëª¨ë¸ í•™ìŠµ ì „ ì›ë˜ vocab í¬ê¸°
print("ğŸ“Œ model embedding size:", model.get_input_embeddings().num_embeddings) # ìŠ¤í˜ì…œ í† í° ì¶”ê°€ í›„ í•™ìŠµì— ì“°ì˜€ì„ë•Œì˜ í¬ê¸°
print("ğŸ“Œ tokenizer.pad_token_id:", tokenizer.pad_token_id)
print("ğŸ“Œ tokenizer.eos_token_id:", tokenizer.eos_token_id)
print("ğŸ“Œ tokenizer.special_tokens_map:", tokenizer.special_tokens_map)

# trainer.model.save_pretrained(SAVE_PATH)
# trainer.processing_class .save_pretrained(SAVE_PATH)
# ëª¨ë¸ ì €ì¥
trainer.save_model(SAVE_PATH)
print(f"ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")

# ì €ì¥ í›„ í™•ì¸
print("\n===== ì €ì¥ í›„ ë””ë ‰í† ë¦¬ íŒŒì¼ ëª©ë¡ =====")
print(os.listdir(SAVE_PATH))
print(f"tokenizer.model íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(os.path.join(SAVE_PATH, 'tokenizer.model'))}")

"""
ë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ë‹ˆ tokenizer.model ì´ ì €ì¥ë˜ì§€ ì•Šê³  ìˆì—ˆë‹¤. Autoê°€ ì•„ë‹Œ LlamaTokenizer ë¥¼ ì¨ì•¼ ì €ì¥ë˜ëŠ” ê²ƒ ê°™ë‹¤ê³ ..
"""

# python FY2025LLM/test/finetuning.py