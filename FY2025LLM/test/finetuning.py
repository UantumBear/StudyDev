""" Llama 3.2 1B íŒŒì¸íŠœë‹

# python FY2025LLM/test/finetuning.py
"""
from transformers import default_data_collator
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from transformers import LlamaTokenizerFast
from datasets import load_dataset
from transformers import EarlyStoppingCallback # í•™ìŠµ ì¡°ê¸°ì¢…ë£Œ
import torch
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
from trl import SFTTrainer, SFTConfig
import json
import os

# ê²½ë¡œ ì„¤ì •
MODEL_PATH = "FY2025LLM/models/llama3.2-1B-hf"
DATASET_PATH = "FY2025LLM/data/converted/CarrotAI/cleaned_llama3_dataset.jsonl"
SAVE_PATH = "FY2025LLM/models/llama3.2-1B-hf/finetuned/model_v1"
os.makedirs(SAVE_PATH, exist_ok=True)

# GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸, í™˜ê²½ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì„±ëŠ¥ í™•ì¸
print(torch.cuda.get_device_name(0))
print(f"Allocated: {round(torch.cuda.memory_allocated(0)/1024**3, 1)} GB")
print(f"Cached:    {round(torch.cuda.memory_reserved(0)/1024**3, 1)} GB")

# tokenizer & model ë¡œë“œ : ë¡œì»¬ ë””ë ‰í† ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = LlamaTokenizerFast.from_pretrained(
    MODEL_PATH,
    use_fast=True,
    local_files_only=True
)

tokenizer.pad_token = tokenizer.eos_token # íŒ¨ë”© ì„¤ì •
# ê³µì‹ LLaMA 3.2ìš© chat template ìˆ˜ë™ ì‚½ì…
tokenizer.chat_template = (
    "{% for message in messages %}"
    "{% if message['role'] == 'system' %}<|start_header_id|>system<|end_header_id|>\n{{ message['content'] }}\n"
    "{% elif message['role'] == 'user' %}<|start_header_id|>user<|end_header_id|>\n{{ message['content'] }}\n"
    "{% elif message['role'] == 'assistant' %}<|start_header_id|>assistant<|end_header_id|>\n{{ message['content'] }}\n"
    "{% endif %}{% endfor %}"
)


# set CUDA_LAUNCH_BLOCKING=1 ì„ í„°ë¯¸ë„ì—ì„œ ì„¤ì •í•˜ê²Œ ë””ë²„ê¹…


# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    local_files_only=True,
    torch_dtype=torch.float16
    # device_map="auto" # vRAM ë¶„ì‚° ë° ëª¨ë¸ ë¶„í•  ë¡œë”©ì— ìœ ë¦¬í•˜ë‹¤ê³  í•œë‹¤. TODO ê³µë¶€í•˜ê¸°
    # accelerateë¥¼ ì‚¬ìš©í•´ì„œ ìë™ìœ¼ë¡œ GPUì™€ CPUë¥¼ ë‚˜ëˆ ì„œ ë¡œë”© ... Trainerì™€ ì¶©ëŒí•˜ì—¬ ì£¼ì„
    # ëª¨ë¸ì„ 32ë¡œ ë¶€ë¥´ê³ , Trainer ì—ì„œ 16ìœ¼ë¡œ ë³€í™˜í•˜ë ¤ê³  í•˜ë©´ CUDA Memory out ì´ ë‚œë‹¤.
)
model.resize_token_embeddings(len(tokenizer))
# ëª¨ë¸ì˜ ì²« ë²ˆì§¸ íŒŒë¼ë¯¸í„° dtype ì¶œë ¥ (float í™•ì¸ìš©ë„) ... Loss nan ì´ ë°œìƒí•˜ì—¬ í™•ì¸
print(next(model.parameters()).dtype) # ê¸°ë³¸: torch.float32
model.to(device)  # ì§ì ‘ GPUë¡œ ì´ë™

# ë°ì´í„°ì…‹ ë¡œë“œ (JSONL)
dataset = load_dataset("json", data_files=DATASET_PATH)
# í•™ìŠµ:ê²€ì¦ = 80:20 ë¶„í•  (shuffle í¬í•¨)
split_dataset = dataset["train"].train_test_split(test_size=0.2, seed=42)
train_dataset = split_dataset["train"]
eval_dataset = split_dataset["test"]



# # ì²« ë²ˆì§¸ ìƒ˜í”Œ ì¶œë ¥
# print("dataset[0]:")
# print(dataset[0])
# chat_text = tokenizer.apply_chat_template(dataset[0]["messages"], tokenize=False)
# print("chat_text:")
# print(chat_text)



MAX_LENGTH = 2048
MIN_LENGTH = 500
token_lengths = []  # ë¶„ì„ìš©


# Tokenize í•¨ìˆ˜
def tokenize(batch):
    """
    LaMA3.2 ëª¨ë¸ì€ Supervised Fine-Tuning (SFT) ì‹œ ëª¨ë¸ì´ assistant ì‘ë‹µë§Œ í•™ìŠµí•˜ë„ë¡ í•´ì•¼ í•œë‹¤ê³  í•œë‹¤.
    labels ì„¤ì • ì‹œ userì˜ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ () í•˜ê¸°
    """
    results = {"input_ids": [], "attention_mask": [], "labels": []}
    assistant_marker = "<|start_header_id|>assistant<|end_header_id|>"

    for messages in batch["messages"]:
        chat_text = tokenizer.apply_chat_template(messages, tokenize=False)

        try:
            # 1. assistant ë¸”ë¡ ìœ„ì¹˜ ì°¾ê¸°
            assistant_marker = "<|start_header_id|>assistant<|end_header_id|>"
            if assistant_marker not in chat_text:
                continue
            # ë¬¸ë§¥ ì¶”ì¶œ
            start = chat_text.index(assistant_marker)
            context = chat_text[max(0, start - 800):]

            # í† í°í™”
            enc = tokenizer(context, padding="max_length", max_length=MAX_LENGTH, truncation=True)
            input_ids = enc["input_ids"]
            attention_mask = enc["attention_mask"]
            labels = input_ids.copy()

            # ë§ˆìŠ¤í‚¹ ì²˜ë¦¬
            prefix_len = len(tokenizer(context[:context.index(assistant_marker)])["input_ids"])
            labels[:prefix_len] = [-100] * prefix_len

            if all(l == -100 for l in labels):
                continue

            results["input_ids"].append(input_ids)
            results["attention_mask"].append(attention_mask)
            results["labels"].append(labels)

        except Exception:
            continue

        return results if results["input_ids"] else None

# ì „ì²˜ë¦¬ ì ìš©
tokenized_train_dataset = train_dataset.map(
    tokenize,
    batched=True,
    remove_columns=["messages"],
    desc="Tokenizing train dataset"
)
tokenized_eval_dataset = eval_dataset.map(
    tokenize,
    batched=True,
    remove_columns=["messages"],
    desc="Tokenizing eval dataset"
)




## ì§ì ‘ torch ë¡œ train ì„ í•˜ë©´ ê³„ì† loss nan ì´ ë°œìƒí•´ì„œ ì¼ë‹¨ ì‚­ì œ..
# DataLoader
# collator = DataCollatorForLanguageModeling(
#     tokenizer=tokenizer,
#     mlm=False
# )
# DataCollatorForLanguageModelingì€ ë³´í†µ MLM(Masked Language Modeling)ìš©ì´ë©°, Causal LMì—ëŠ” ë§ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.
# Dataloader
# collator = default_data_collator
# dataloader = DataLoader(tokenized_dataset, batch_size=1, shuffle=True, collate_fn=collator) # batch 4 ë¡œëŠ” GPUê°€ í„°ì§„ë‹¤.

# Optimizer & AMP
# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6) # 1e-5 ë¡œ í•˜ì ê³„ì†í•´ì„œ Loss: nan ì´ ë°œìƒí–ˆë‹¤.
# scaler = GradScaler(device='cuda')  # <-- ë””ë°”ì´ìŠ¤ ëª…ì‹œ!

# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
# torch.cuda.empty_cache()

# í•™ìŠµ ì „ í™•ì¸
print(f"Model dtype: {model.dtype}")
print(f"Model device: {next(model.parameters()).device}")

# í•™ìŠµ ì‹œì‘
print("Start training...")



# Trainer í•™ìŠµ íŒŒë¼ë¯¸í„°
# training_args = TrainingArguments(
#     output_dir=SAVE_PATH,
#     per_device_train_batch_size=1,
#     gradient_accumulation_steps=1,
#     num_train_epochs=1,
#     logging_steps=10,
#     save_steps=100,
#     save_total_limit=1,
#     learning_rate=5e-6,
#     # bf16=False, # bfloat16 â†’ False
#     # fp16=True,  # AMP ì ìš© (ìë™)
#     report_to="none"
# )
training_args = TrainingArguments(
    output_dir=SAVE_PATH,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=1,
    num_train_epochs=1,
    logging_steps=10,
    save_steps=100,
    eval_strategy="steps", # í•™ìŠµ ë„ì¤‘ ì£¼ê¸°ì ìœ¼ë¡œ í‰ê°€(evaluation)ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ ì„¤ì •
    eval_steps=100,
    save_total_limit=1,
    load_best_model_at_end=True,
    metric_for_best_model="loss",
    greater_is_better=False,
    learning_rate=5e-6,
    report_to="none"
)
# SFTTrainer ì •ì˜
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    processing_class=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] # ê²€ì¦ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•ŠëŠ” epochê°€ 2ë²ˆ ì—°ì† ë°œìƒí•˜ë©´ í•™ìŠµì„ ì¤‘ë‹¨.
)

# í•™ìŠµ
trainer.train()




# ëª¨ë¸ ì €ì¥
print("===== ë‚´ë¶€ backend í™•ì¸ =====")#
print("Tokenizer class:", tokenizer.__class__)
print("Has backend_tokenizer:", hasattr(tokenizer, "backend_tokenizer"))
print("Contains tokenizer.model?", os.path.exists(os.path.join(SAVE_PATH, "tokenizer.model")))
print("===== ëª¨ë¸ ì €ì¥ ì§ì „ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ì •ë³´ í™•ì¸ =====")
print("ğŸ“Œ tokenizer vocab size:", tokenizer.vocab_size)
print("ğŸ“Œ model embedding size:", model.get_input_embeddings().num_embeddings)
print("ğŸ“Œ tokenizer.pad_token_id:", tokenizer.pad_token_id)
print("ğŸ“Œ tokenizer.eos_token_id:", tokenizer.eos_token_id)
print("ğŸ“Œ tokenizer.special_tokens_map:", tokenizer.special_tokens_map)

trainer.model.save_pretrained(SAVE_PATH)
trainer.processing_class.save_pretrained(SAVE_PATH)
print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")
tokenizer.save_pretrained(SAVE_PATH)
print(f"í† í¬ë‚˜ì´ì € ì €ì¥ ì™„ë£Œ: {SAVE_PATH}")

"""
ë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ë‹ˆ tokenizer.model ì´ ì €ì¥ë˜ì§€ ì•Šê³  ìˆì—ˆë‹¤. Autoê°€ ì•„ë‹Œ LlamaTokenizer ë¥¼ ì¨ì•¼ ì €ì¥ë˜ëŠ” ê²ƒ ê°™ë‹¤ê³ ..
"""

# python FY2025LLM/test/finetuning.py