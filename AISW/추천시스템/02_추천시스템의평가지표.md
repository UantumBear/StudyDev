# 1. 평가 방식 개요 (Evaluation Paradigms)

추천 시스템은 보통 **오프라인 평가** 로 모델의 성능을 판단하고,  
**정확도(accuracy)** 를 주 기준으로 사용한다.  

# 2. 정확도 기반의 평가 지표 (Accuracy Metrics)

## 1. MSE / RMSE / MAE
### MSE (Mean Squared Error)
**평균 제곱 오차** 
예측 평점과 실제 평점의 차이를 제곱한 것.  
평균 값이 작을수록 예측 정확도가 높다.
### RMSE (Root MSE) 
**평균 제곱근 오차**
`큰 오차 (Outlier)에 더 민감`하다.
### MAE (Mean Absolute Error)
**평균 절대 오차**
절대값 차이의 평균으로, 해석이 직관적이다.

## 2. Precision & Recall
추천 문제를 **분류(Classification)** 관점에서 평가하는 것이다.
Precision : 추천된 것 중 `실제로 맞은 비율`
Recall : `실제 좋아한 것 중` 추천된 비율

Precision과 Recall은 Trade-off 관계 존재 이다.  
F1-score와 Fbeta-score 는 Precision과 Recall의 조화 평균으로,  
두 지표의 균형적 성능을 평가한다.

## 3. AUROC / AUC
Area Under ROC (curve)
**임계값(Threshold)**의 변화에 따라, Precision과 Recall이 변하기 때문에  
전체 구간에서 `모델의 분류 성능을 종합적으로 평가`하는 지표가 된다.  
Imbalanced Data 문제에 강하다. (positive와 negative의 비율 불균형을 완화한다.)  

AUC는 모델이 임의의 positive 를 negative 보다 높게 점수를 줄 확률을 의미한다.  

# 3. 순위 기반 평가 (Ranking Metrics)
## (1) Hit Rate@k
Tok-k 추천 중 사용자가 좋아한 항목이 하나라도 포함되면 1점
단순하지만, 모델의 추천 성공 여부를 직관적으로 평가한다.  

## (2) Precision@k / Recall@k  
Precision@k : 상위 k개의 추천 중 정답의 비율  
Recall@k : 실제 좋아한 항목 중 상위 k개에 포함된 비율  

## (3) Average Precision@k (AP@k) & MAP@k
추천 순위의 품질을 함께 고려
AP@k :
MAP@k : 모든 사용자에 대한 AP@k의 평균

## (4) DCG@k / NDCG@k
DCG@k (Discounted Cumulative Gain) : 
상위 순위 항목일 수록 더 큰 가중치를 부여한다.  
NDCG@k : DCG를 이상적 (DCG)로 나눈 정규화 버전으로, 서로 다른 시스템 간 비교가 가능하다.

NDCG는 `추천 결과의 순서 중요도`를 반영하는, 정규화된 순위 지표이다.  

## (5) MRR (Mean Reciprocal Rank)
사용자가 가장 먼저 맞춘 정답 항목 위치(rank)를 기준으로 계산한다.  
빠르게 올바를 추천을 제시할수록 높은 점수이다.  

# 4. 평가 지표의 한계
## 단일 지표 의존의 문제
하나의 지표 MAE, RMSE 만으로는, 모델 성능을 종합적으로 평가하기에 부족하다.  
여러 지표를 조합 평가해야 한다.

## 데이터 불균형 및 희소성 문제
대부분의 사용자-아이템 데이터는 **롱테일 분포**를 가진다.  
인기 아이템에 대한 평가는 많고, 비인기 아이템에 대한 평가는 적다. 

## 비정량적 평가 항목의 중요성
추천 시스템은 정확도 외에도, 다양한 품질 요소를 고려해야 한다.  
- Coverage : 다양한 아이템을 얼마나 다루는가
- Novelty : 새로운 아이템을 얼마나 제시하는가
- Diversity : 추천 목록이 얼마나 다양한가
- Serendipty : 예상치 못한 즐거움을 주는가
- Scalability : 대규모 데이터에서도 빠르게 작동하는가

