# 2장. n-gram Language Model
### n-그램 언어 모델이란? 
    단어 시퀀스의 확률을 계산하는 확률적인 모델.
    각 단어가 이전 단어와 어떤 관계에 있는지를 나타내는 조건부 확률을 사용하여
    문장의 발생 가능성을 계산한다.
### Markov 가정
    언어 모델에서 전체 긴 문맥을 고려하는 대신, 
    최근의 k 개 단어만을 사용해 다음 단어를 예측하는 방식을 말한다.
    이는 모델이 추정해야 할 매개변수의 수를 줄여주는 효과가 있다.
### 확률 추정
    n-그램에서 조건부 확률을 계산할 때
    최대 우도 추정을 사용한다.
    이는 훈련 데이터에서 관찰된 빈도수를 기반으로 한다.
### 언어 생성
    n-gam 모델을 사용하여 단어를 생성할 때에는,
    첫 단어는 임의 선택 -> 그 이후의 단어는 조건부 확률로 선택한다.
### 평가 방법
    혼란도(perplexity)를 언어 모델의 성능 평가 지표로 사용한다.
    모델이 다음 단어를 얼마나 잘 예측하는 지를 측정하는 것이다.
    혼란도가 낮을수록 좋은 모델이다.
### 스무딩 기법
    n-그램 모델의 문제 중 하나는, 훈련 데이터에 없던 새로운 n-그램이 test data에 등장할 때,
    확률이 0이 되는 문제이다. 
    이를 해결하기 위해 라플라스 스무딩, 절대 할인법 등 스무딩 기법을 사용하여
    모든 단어가 0의 확률을 갖지 않도록 조정한다.

## 개념 1. 

### Top-K 샘플링
    Top-K 샘플링이란?
    확률이 가장 높은 상위 k개의 후보 단어들 중에서,
    무작위로 하나를 선택하는 방법이다.
    
    예를 들어 k=5 이면,
    확률이 높은 상위 5개의 단어를 선택 후,
    그 안에서 랜덤으로 단어를 선택한다.

### Top P 샘플링
    Top-P 샘플링이란?
    (확률이 큰 순서부터 누적시켜서)
    확률의 누적합이 p 이상이 되는 상위 단어들을 선택하는 방법이다.
    
    예를 들어 p=0.65 이면,
    확률의 누적합이 0.65 이상이 되는 단어들을 선택하고,
    그 안에서 랜덤으로 하나를 선택한다.

apple	0.12
banana	0.15
cherry	0.07
date	0.06
elderberry	0.08
fig	0.18
grape	0.10
honeydew	0.05

```python
# top-k 샘플링 k=5 로 생성될 수 있는 단어들:
fig > banana > apple > grape > elderberry

# top-p 샘플링 p=0.65 로 생성될 수 있는 단어들:
fig 0.18 + banana 0.15 + apple 0.12 + grape 0.10 
+ elderberry 0.08 + cherry 0.07  
```

---

## 개념 2.
### 언어 모델 (Language Model)
#### 유니그램
    단어 하나의 빈도만을 고려하는 모델
    P(the cat sat on) = P(the) * P(cat) * P(sat) * P(on)

#### 바이그램
    두 단어가 연속해서 나타날 확률을 계산하는 모델
    현재 단어가 바로 이전 단어에 의존한다고 가정하는 모델이다.
    P(the cat sat on) = P(the) * P(cat|sat) * P(sat|cat) * P(on|sat)
    $$
    P(w_n \mid w_{n-1})
    $$
    단어 Wn-1 이 주어졌을 때, 단어 Wn 이 나타낼 확률을 계산한다.

#### 트라이그램
    세 단어가 연속해서 나타날 확률을 계산하는 모델
    P(the cat sat on) = P(the) * P(cat|the) * P(sat|the cat) * P(on|cat sat)
    $$
    P(w_n \mid w_{n-2}, w_{n-1})
    $$
    두 개의 이전 단어 Wn-2, Wn-1 이 주어졌을 때, Wn이 나타날 확률을 계산한다.

#### 추가 스무딩 Additive Smoothing 
    스무딩이란?
    언어 모델을 구축할 때, 훈련 데이터에서는 관찰되지 않은 단어 조합이 있을 수 있다.
    이런 경우에 확률이 되지 않도록 하는 기법이 Smoothing 이다.

    Additive Smoothing 은 라플라스 스무딩이라고도 하는데,
    모든 빈도에 1을 더해줌으로써 확률이 0이 되는 것을 방지한다.

    아래는 바이그램 모델에서, n-1 번째 단어가 주어졌을 때,
    n번째 단어가 나타날 확률을 계산하는 공식이다.
    +1 부분이 스무딩을 하는 부분이며,
    |V| 는 전체 단어 집합의 크기 == 고유한 단어들의 총 개수를 의미한다.

    $$
    P(w_n \mid w_{n-1}) = \frac{count(w_{n-1}, w_n) + 1}{count(w_{n-1}) + |V|}
    $$

#### PPL (Perplexity, 혼란도)
    혼란도는 언어 모델의 성능을 측정하는 중요한 지표이다.
    Perplexity는 모델이 주어진 문장을 얼마나 잘 예측하는지를 나타내며,
    낮은 Perplexity 값일수록 모델이 더 좋은 성능을 보인다.
    
##### 유니그램에서의 PPL
    Perplexity = P(w1, w2, ..., wn) ^ (-1/n)
    여기서 w1, w2, ... wn 은 문장을 구성하는 각각의 단어들이다.
    n은 문장을 구성하는 단어의 총 개수이다.

    예를 들어, 유니그램 빈도가 아래와 같다면,
    Unigram, Count
    I, 20
    eat, 10
    want, 12
    to, 25
    car, 6
    코퍼스에 등장한 총 단어 수 Total Count
    = 20 + 10 + 12 + 25 + 6 = 73 이라고 한다.

    P(I) = 20/73
    P(eat) = 10/73
    P(want) = 12/73
    P(to) = 25/73
    P(car) = 6/73

    이 경우 "I want to eat car" 라는 문장이 나올 확률은
    P(I want to eat car) 
    = P(I) * P(want) * P(to) * P(eat) * P(car)

    Perplexity = P(문장)^(-1/문장에 나오는 총 단어 수)
    = P(I want to eat car) ^ (-1/5)

    !!n 은 고유 갯수도, 코퍼스 도 아닌, 확률을 구하고자 하는 해당 문장에 나오는 총 단어 갯수를 말한다.

##### TODO 바이그램에서의 PPL 구해보기

##### TODO 트라이그램에서의 PPL 구해보기
    

```python
# "I want to eat car 의 PPL (Perplexity) 점수 계산하기."
Unigram, Count
I, 20
eat, 10
want, 12
to, 25
car, 6

Bigram, Count
I want, 4
want to, 8
to eat, 5
eat car, 0

Trigram, Count
I want to, 3
want to eat, 4
to eat car, 0
```


## 3장. Text Classification 텍스트 분류

### 텍스트 분류 기법
#### 룰 기반 분류
    특정 단어가 존재하면, 해당 클래스에 할당하는 규칙 기반 분류 기법.
    규칙 정의가 어렵다는 단점.
#### 나이브 베이즈 Naive Bayes 분류 기법
    나이브 베이즈란, 베이즈 정리를 사용한 확률 기반 분류 모델을 말한다.
    각 클래스의 사전 확률과, 문서 내 단어가, 주어진 클래스에 속할 조건부 확률을 계산해서
    가장 높은 확률을 가진 클래스로 분류한다.
    
    나이브 베이즈 분류기는 적은 양의 데이터로도 효과적으로 학습하는 분류 모델이나,
    각 단어가 서로 독립적이라고 가정하는 부분이 현실적이지 않고,
    클래스가 불균형할 수록 성능이 저하된다.
    라플라스 스무딩을 통해, 일부 단어가 출현 빈도가 없는 문제를 해결한다.

#### 로지스틱 회귀 Logistic Regression 분류 기법
    이진 혹은 다중 클래스 분류에서 사용하는 기법이다.
    입력을 특성 벡터로 변환한 뒤, 클래스 확률을 예측한다.
    손실 함수 Cross Entropy 를 최적화 하여, 모델의 가중치를 학습한다.
    
    로지스틱 회귀는 강력한 지도 학습 모델이다.
    크로스 엔트로피 손실 함수를 사용해 학습하고, 경사 하강법으로 모델의 가중치를 최적화 하며,
    과적합을 방지하기 위해서는 L2 정규화를 사용한다.


## 개념 3. 


### 나이브 베이즈 분류기(Naive Bayes Classifier)

    나이브 베이즈 분류기를 사용하여
    주어진 문장의 감정이 긍정적인지 부정적인지 판별하기.

    나이브 베이즈 분류기는 베이즈 정리를 기반으로 한 분류 알고리즘 이다.
    모든 특징이 독립적이라 가정하고,
    주어진 데이터가 특정 클래스에 속할 확률을 계산한다.

    $$
    P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
    $$

    𝑃(𝐶∣𝑋): 주어진 데이터 X가 클래스 𝐶일 확률

- 나이브 베이즈의 기본 개념을 알고 있는가?
- 분류기를 사용하여 확률 계산을 할 줄 아는가?
- 감정 분석에서 나이브 베이즈 사용을 하려면 어떻게 해야 하는가?

### 감정 분석 (Sentiment Analysis)

    감정 분석은 텍스트에서 감정(긍정/부정)을 판별하는 기법이다.
    
    텍스트 전처리가 중요하다.
#### 불용어 (Stopwords)
    불용어란 의미상 중요한 역할을 하지 않는 단어로, 전처리 단계에서 제거한다.

### 스무딩 (Smoothing)

#### 라플라스 스무딩 (Additive Smoothing)
    라플라스 스무딩이란, 새로운 데이터에서 0으로 예측되는 확률을 방지하기 위한 기법이다.
    각 사건의 발생 횟수에 일정 값을 더해 확률을 0이 되지 않도록 조정한다. 

    $$
    P(w|c) = \frac{count(w, c) + \alpha}{count(c) + \alpha \cdot |V|}
    $$

    𝛼: 스무딩 파라미터

### 나이브 베이즈를 이용한 감정 분석

Step 1. 불용어를 문장에서 제거하여 전처리
Step 2. 나이브 베이즈 분류기를 사용해 해당 문장의 확률 계산
Step 3. 해당 문장이 긍정인지 부정인지 판별

"the lecture was long and wonderful"

카테고리	문장	불용어
"the meal was absolutely wonderful"	"식사는 정말 훌륭했다"	the
"truly enjoyed the insightful lecture"	"정말 통찰력 있는 강의를 즐겼다"	was
"lecture was wonderful and friendly"	"강의는 훌륭하고 친절했다"	and
"meal was bland and disappointing"	"식사는 밋밋하고 실망스러웠다"	and
"the lecture was tedious and long"	"강의는 지루하고 길었다"	the



## 개념 4. 
```python
# 로지스틱 회귀 모데을 사용해서 훈련 데이터셋 Dtrain의 세 개의 데이터 샘플에 대한 예측된 확률이 주어졌을 때,
# 크로스 엔트로피 손실 계산하기. (Cross Entropy Loss)

예측 확률, 실제 레이블
p(y=1 | x1) = 0.8, y=1
p(y=1 | x2) = 0.6, y=1
p(y=1 | x3) = 0.3, y=0
```
### 크로스 엔트로피 손실 (Cross-Entropy Loss)
크로스 엔트로피 손실이란, 
예측된 확률과 실제 레이블 간의 차이를 측정하는 방법이다.
Cross Entropy Loss 가 작을수록, 모델의 예측이 실제 label 과 가깝다는 것을 의미한다.

크로스 엔트로피 손실 공식:
$$
L = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
$$


## 개념 5.

### 크로스 엔트로피 손실의 값이 가질 수 있는 범위
"0 이상, 무한대"
TODO 식 이해하기...

***
***
***

## 4장, 5장 Word Embedding 단어 임베딩
### Word Embedding 이란?
    단어를 벡터로 표현하여 의미를 나타내는 방법을 말한다.
    각 단어는 벡터 공간에서 유사한 의미를 지닐 수록 가깝게 배치된다.
### 분포 가설 (Distributional Hypothesis)
    단어의 의미는 그 단어가 사용되는 '문맥'에서 정의된다는 가설이다.
    같은 문맥에서 자주 등장하는 단어들은 비슷한 의미를 가진다.
### 코사인 유사도 (Cosine Similarity)
    두 벡터의 유사성을 측정하는 방법으로, 벡터 사이의 각도를 이용한다.
    값이 1에 가까울수록 두 벡터는 더 유사한것으로 간주된다.
### PPMI (Positive Pointwise Mutual Information)
    단어와 단어 사이의 빈도 기반 상관관계를 나타내는 방법을 말한다.
    PPMI는 PMI의 음수 값을 0으로 대체하여, 
    단어의 의미를 더 안정적으로 표현할 수 있도록 한다.
### Sparse vs Dense Vectors
    희소벡터란, 대부분의 값이 0 인 긴 벡터로, 일반적으로 단어와 단어 공기(count) 행렬을 통해 생성된다.
    밀집벡터란, 짧고 실제 값으로 이루어진 벡터로, 희소벡터보다 더 많은 의미를 압축해서 표현할 수 있다.
    밀집벡터가 더 나은 일반화를 제공한다.
### 단어 임베딩 생성 방법
    카운트 기반 방법:
    단어-단어 공기 행렬을 분해하여, 벡터를 생성하는 SVD 같은 방법이 있다.
    예측 기반 방법:
    단어가 특정 문맥에서 나타날 확률을 예측하는 방식, Word2Vec, Glove, FastTest 같은 기법이 있다.
### 학습된 단어 임베딩
    학습된 단어 임베딩은 이미 다양한 언어와 데이터셋에서 사용할 수 있도록 훈련된 모델들이 제공된다.
    예를 들면 Word2Vec, Glove, FastText 들은 각기 다른 데이터셋을 사용하여 학습되었다.

### Word Embedding 의 이론적 배경
#### 분포 가설
    유사한 단어는 유사한 문맥에서 발생한다는 가설
#### Word2Vec 모델
    단어를 실수 값 벡터로 표현하는 방법,
    단어의 의미를 벡터 공간에서 유사한 단어들과 가깝게 위치시키는 것이 목표이다.
### Word2Vec 의 작동 방식
#### skip-gram 모델
    스킵 그램 모델은, 주어진 단어를 중심으로,
    해당 단어의 문맥에 있는 단어들을 예측한다.
    각 단어는 중심 단어와 그 주변 단어들의 관계를 학습한다.
#### 목표 함수
    skip-gram 모델의 목적은, 주어진 단어가 문맥에서 발생할 확률을 최대화 하는
    파라미터를 찾는 것이다.

### Negative Sampling
    문맥에 있는 모든 단어에 대해 확률을 계산하는 것은 매우 비용이 많이 들며,
    현실적으로 불가능하다.
    이를 해결하기 위해, 네거티브 샘플링 기법을 사용한다.
    이는 일부 단어들만 랜덤으로 샘플링하여, 확률을 계산하는 방식이다.

### CBOW (Continous Bag of Word)
    CBOW 모델은, skip-gram 모델과는 반대로,
    주변 단어로 부터 중심 단어를 예측하는 방식이다.
    이 모델 또한 Word2Vec의 한 방식인데,
    문맥 단어들의 평균을 사용하여 중심 단어를 예측한다.

### Word Embedding의 평가 방법
#### 내재적 평가
    특정한 하위 작업(단어 유사도 계산 등) 으로 단어 임베딩의 성능을 평가
### 외재적 평가
    학습된 단어 임베딩을 실제 NLP 시스템에 적용하여, 성능을 평가하는 방법
    이 방식은 더 오래 걸리지만, 실제 성능 향상을 평가하는 중요한 척도가 된다.

### Skip-gram의 다른 도메인 적용
    POI 임베딩 (Point of interest)
    사용자 체크인 기록과 같은 데이터를 활용하여, POI 간의 관계를 학습하는데에
    skip-gram 모델을 적용할 수 있다.



## 개념 6.
```python
# 동시 발생 카운트 벡타 u, v 를 기반으로
# 코사인 유사도인 cosine(u,v) 를 사용하여 단어 유사도를 측정할 때,
# 가능한 값의 범위는 어떻게 되는가?
```
### 코사인 유사도 (Cosine Similarity)
    코사인 유사도란, 두 벡터 사이의 각도를 이용해서 유사도를 측정하는 방법을 말한다.
    값이 클수록 두 벡터가 유사하며, 값이 작을수록 두 벡터가 서로 다르다.

    코사인 유사도는 벡터의 방향만을 고려하며, 크기는 고려하지 않는다.
    이 때문에 단어간의 유사도와 단어 벡터 간의 유사도를 측정하는 데에 자주 사용된다.
$$
\text{cosine}(u, v) = \frac{u \cdot v}{\|u\| \|v\|}
$$





## 개념 7.
```python

```
### Word2Vec 알고리즘
    Word2Vec 이란, 단어를 벡터로 변환하여 단어 간의 관계를 벡터 공간에서 학습하는 방법이다.
    
    Word2Vec 에는 크게 CBOW 모델과 Skip-gram 모델이 있다.

#### CBOW (Continuous Bag of Words) 
    주변 단어 (context)를 보고, 중심 단어 (center word) 예측하기

#### Skip-gram 
    중심 단어를 보고, 주변 단어 예측하기

### 목적 함수 (Objective Function)
    Word2Vec의 Skip-gram 모델에서는, 
    중심 단어 Wt 가 주어졌을 때,
    주변 단어 Wt+j 를 예측하는 확률을 최대화하는 것이 목적이다.

### 확률 모델
    각 주변 단어가 중심 단어에 대해 나타날 확률을, 최대화하려는 목적 함수이다.
    확률 모델은,
    컨텍스트 윈도우 내의 각 주변 단어들이 중심 단어와 얼마나 밀접하게 연결되어 있는지를 나타낸다.
    주어진 중심 단어를 기준으로, 주변 단어들이 특정 확률로 예측된다.

### Log-likelihood
    Word2Vec 의 목적 함수는 일반적으로 Log-likelihood (로그 우도)로 변환된다.
    이를 통해, 곱셈 연산을 덧셈 연산으로 변환해서 계산의 안정성과 효율성을 높인다.

## 6장. Sequence Model 순차 모델

### 순차 모델이란?
    순차 모델은, 입력이 순서에 따라 이루어진 데이터를 다루는 모델을 말한다.
    품사 태깅 (POS) 이나, 개체명 인식(NER) 등의 작업에서 사용된다.

### 마코프 모델과 히든 마코프 모델 (HMM)
#### 마코프 모델
    각 상태가 이전 상태에만 의존하는 확률 모델을 말한다.
#### 히든 마코프 모델
    관찰 가능한 데이터와 관찰되지 않은 숨겨진 상태 (품사 태그)를 연결하여 모델링 하는 모델이다.
    전이 확률과 방출 확률을 통해, 숨겨진 상태와 관찰된 데이터를 연결한다.

### Viterbi 알고리즘
    HMM에서 가장 가능성 높은 상태 시퀀스를 찾는 동적 프로그래밍 기법이다.

### 빔 서치
    Viterbi 알고리즘의 계산비용이 클 때,
    빔 서치는 경로의 수를 제한하여 계산량을 줄인다.
