Nvidia Bootcamp 2025.05.21 - 2025.05.22

Nvidia GPU - end to end application 가속화


---
**사용자별로 제공되는 클러스터 접속 관리 포털**  
https://axis-raplabhackathon.axisportal.io/apps

1. 이메일을 통해 전달받은 공용 ID/PW 를 통해 로그인한다.  

2. Open web SSH in a new tab 과  
  Use a desktop SSH client 중 데스크탑을 선택한다.   
주피터 환경을 통한 포트포워드 작업이 필요하기 때문

3. 이메일을 통해 전달받은 개별 ID/PW 를 통해 로그인한다.

4. 접속할 클러스터를 선택한다.

5. SSH 접속용 명령어를 알려주는 창이 뜬다.  
  해당 주소를 카피한 후 본인 데스크탑의 cmd 창에 입력하여 접속한다.
  ```linux
 # One time access key for Secure Shell - Connect with terminal   
   ssh ssh.axisapps.io -l [사용자계정해시코드]
```

6. 전달받은 명령어로 쉘 스크립트를 실행한다.  
```linux /mnt/[경로생략]/e2e-llm.sh ```

7. 메뉴를 선택한다.
```linux
  End-to-End LLM BOOTCAMP

1) Run Llama Finetuning Lab           # 첫날
2) Run TRT-LLM Lab                    # 둘째날
R) Reset Llama Finetuning Container   # 접속 안될경우 첫 날 리셋
T) Reset TRT-LLM Container            # 접속 안될경우 둘째 날 리셋
X) Exit
Choose an option (1, 2, R, or X):
```

---

1번을 선택하면,  아래와 같이 뜬다. 3~10분 정도 걸린다.
```linux 
Preparing Llama Container
TRANSFERING AND VERIFYING CONTAINER 
```

시간이 지나자 해시 코드 입력 문구가 떴다. 
ssh 접속 시 사용했던 해시 코드를 입력해준다.
```linux 
Please enter the Axis Hash code: 
```
```linux
Initializing Lab
Prepareing.... 
```
```linux
 Step 1: Please copy the following command into a local terminal
 ssh -L localhost:9999:dgx06:9250 ssh.axisapps.io -l [입력했던해시값]
 Step 2: Copy the following into your web browser:
 http://localhost:9999
 klm-v5xx45@curiosity-headnode:~$
```

이렇게 떴다. 총 창을 두개를 띄워야 한다.
현재 창 유지, 다시 cmd 창을 열고  아래 명령어 입력.
```linux
ssh -L localhost:9999:dgx06:9250 ssh.axisapps.io -l [입력했던해시값]
```

그리고 localhost 에 접속하면 주피터 환경이 나온다.


포트포워딩을 하기 위한 창 ??)

근데 Step 1에서 받은 저 주소를 쳤더니,
klm-v5xx45@curiosity-headnode:~$ channel 3: open failed: connect failed: Connection refused
channel 4: open failed: connect failed: Connection refused

이렇게 뜬다,..

squeue | grep 본인의 아이디
squeue | grep klm-v5xx45   <- 이거 맞나?

---


Agenda
LLM 이 뭔지
Pretraining
Transformer Architecture
Attention 구조
FEFT

-------------------------------
Backgroud

최근에는
Language Modeling (사전학습 LM) - ELMO, BERT, GPT-1/2
LLM - GPT-3/4
-----------------
Scaling LLMs
LLM 은 스케일링이 중요하다.
그당시에는 Masked LLM 이었는데, 
모델사이즈가 커지고, 학습량이 늘어나면 성능이 좋아지고,
그렇게 되면 Zero shot 도 가능하고, 파인튜닝만 하면 성능이 잘 나왔다.
GPT-3, PaLM, Galactica, LLaMA 등.

model size (모델의 깊이, 넓이를 얼마나 크게 했는지,, 16 레이어 24레이어.. dimension 2k 3k..)
data size (학습 데이터, 몇 tera token 을 학습시켰는지. 옛날에는 이미지 몇백만 장이다. 이렇게 말했다면 이젠 전체 토큰을 기준으로)
total compute (GPU 한장을 가지고 학습하긴 어려우니, 한 노드간의 통신도 nv link 를 쓰거나 데이터 병렬화를 사용해서 node scaling)
그리고 노드 사이즈가 커지면 하나의 GPU 안에 들어가지 않는다.
그래서 optimizer 를 할 때 (백워드, 포워드) 옵티마이저도 사이즈가 커서, 메모리를 많이 차지해서, 한장의 GPU  안에 안들어가기 떄문에 model 도 model 병렬화를 해서, dimension 을 자름.
dimension 도 멀티헤더 구조를 사용해서.. 
또 파이프라인 병렬화, 데이터 병렬화 기법 등 다양한 기법을 사용하여
100 billion, 500billion 모델.. 들을 타겟 시간 안에 학습시킬 수 있도록

요즘은 무아이 법칙과 비슷하게, 1년이 지나면 과거모델보다 더 작은 모델로 성능이 좋다.
버전이 올라갈수록 더 작은 모델로 성능은 더 올려서 발전 중인 것.

--------------------------------------
Emergent Abilities of LLMs
LLM 의 능력이 좋아진 것
초창기에는 pretraining 을 하고 파인튜닝을 instuction , reinforce 등 분리가 되었었는데,
최근에는 instruction 에 이용하기 위한 것을 
아예 pretraining 동작에서부터 task 와관련된것을 학습시켜서
더 잘 알아들을 수 있도록..
구글이 제안한 CoT (chain of thought) 기반 프롬프팅 전략 한번 하는게 아니라 30번 100번..
최근에는 셀프 에러 도 가능
Think tag 는 내부적으로 하고, 아닌 것만 웹 ui로 보이게끔 해서
o3 모델처럼. 
민주화가 되었다??
프롬프트를 넣어줄 때 think tag 을 써라. 라고 해서씽크텍을 열고 닫고 할 수 있게
CoT 가 중요한 역할을 하는 중

A brief Illustration for GPT-series Model

딱 ChatGPT 나오기 전까지 instuction 기법은 많이 연구가 되었었음.
deepseek 아론의 경우는  ... 
GPT-4 에서는 reasoning 성능이 갑자기 좋아짐

reforce learning 으로 휴먼피드백을 주는 것
시스템 롤을 아예 없애버리고 채팅 만으로도 알 수 있게끔 하는 그런 기법
in-context learning
code 를 학습했더니 추론 성능이 좋아진다거나, 수학 계산 능력이 좋아진다거나.
quantazation 을 해서 메모리 사용량을 확 줄인다거나.
KV cache 를 사용해서 어떻게 효율적으로 할 지에 대한 연구가 진행되고 있음.

---------------------------------------
Publicly Available LLMs

오픈소스모델
--------------------------------
CLosed Source LLMs
GPT
Anthropic 이 최근 핫함
Closed Source 의 경우에는 사실 token 을 어떻게 사용했는지도 공개가 되어있지 않기 때문에
정확히 어떻게 작동하는지 알 수 없음

-------------------------
Data Source for Pre-training and Fine tuning
예전 모델들을 BERT 가 BookCorpus 를 사용해서 5GB 만으로 충분히 학습이 가능했다.
CommonCrawl 라는건 원래 테라바이트 단위인데, C4로 구글에서 줄여서..


시뮬러벡터 계산하면 한 10분의 1정도는 a 사이트와 b 사이트가 동일한 어느정도 데이터를 중복하고있다고함
그래서 코어 데이터를 모은게 C4 , 다국어
the Pile 이 오픈소스 진영에서 전체 데이터를 다 받을 수 있는 것 800GB

로우데이터만 학습하는게 아니라
instruction alignment ...
test dataset 을 pretrained data set 으로 들어가지 않도록 주의.

--------------
학습을 위한 라이브러리
Transformers, DeepSpeed (MS) , Megatron-LM :: Nvidia 가 표준으로 
JAX 도 인기 있음. pytorch 도 파이토치 컴파일을 이용할 수 있는데, 그래프 기법으로 경우에 따라 모델 수정없이 속도가 2-3배 빨라질 수 있음.
inference 할때는 vLLM...
Llama 4는 FastMoE 사용 중...

-----------------------------------------------------
Data Collection and Preparation
이제 데이터 준비 
풍부한 데이터를 준비해야 한다.
국산 LLM 만들기. 충분히 가능함.
common crawl 에서 시작해서, 여기에 book 데이터 커스텀 데이터 추가해서..
Code 만 넣어서 코드에 강하게 할 수도 있고
Galactica (할루시네이션이 좀 문제였으나 좋았던 모델임)

-----------------------
Data Scheduling
데이터를 학습하다 보면 과거정보가 사라지는 문제가 있음. (거의 완벽하게 잃어버리는..)
learning rate 를 잘못 조절하면 아예 터져버리는..
Stage 1 에서 웹데이터를 많이 쓰고
Stage n 에서는 Coding, Chatting, Debuging정보 등이 추가되면 잘 되는..
물론, 완전히 그렇게 갈아끼우면 안되고 data mixture 를 해서 조금씩 섞어서..

---------------------------------------
Text Data Classification
대화 데이터는 사람의 데이터를 구하기 쉽지 않기 때문에
LLM Conversation data set 을 사용
최근에는 신세틱..? 을 사용해서, 데이터를 늘려서, 학습에 이용 

--------------------------------------
Data Preprocessing

데이터 준비과정 Nvidia의 솔루션 Nimo 에서 이런 것들을 제공함.
Raw Corpus, jsonsL 로 뽑거나 정리하는 작업들..
반복되어있는 센텐스를 줄이고 정제
코사인 유사도를 전수조사 해서 이거 삭제는 무조건 해야 함.

------------------------------------
Tokenization Methiods
토크나이저는 워낙 빨라서 GPU사용까지는 필요 없지만,
더 빠르게 처리할 수 있도록, (0.001초라도 쌓이면 학습시간에 영향이 있기 때문에)
그런것때문에 토크나이저 GPU 가속도 있다!

-----------------------------------------------
Scalable Training Techniques
파이프라인 병렬화
모델을 어떻게 자를지에 대한 텐서 병렬화
tensor 병렬화 - 멀티헤드 어텐션을 이용해서 헤드 사이즈 기준으로 쉽게 자름

옵티마이저쪽에 ZeRO 사용하는거 있고
Nvidia 가 강점을 가지고 있는건 PRecision Training
16비트, 풀 16비트로 트레이닝하다보면 잘 안되기도 함.
32비트로저장하고 16비트로 계산해서, 128로 곱했다 나눴다 하면
16, 32비트를 왔다갔다 하면서 저장, 계산하는데 이런걸 Mixed PRecision Training 이라고 함.
이게파이토치에 내장이 되어있음. 
Brain Float 이용하면..
흠
믹스트 프리시즌은 그래서 안 쓸 이유가 없음. 
MS가 제안한대로 터널이 3비트만으로 처리되기도 함?? (6비트)
장점이 곱하기 0이되면 계산안해도 되는 것

--------------------------------------------------------------
TRANSFORMER ARCHITECTURE

트랜스포머구조
코잘 랭귀지 모델
트랜스포머는 앞의 인코더는 bidirection 앞에서 뒤를 다 봐서 인코딩을 하고 (문장의 클래스를 하거나, 번역모델쓸때 이용함)
디코딩은 미래를 보지 않는 토큰을 만듦
Causal 은 하나의 인풋이 들어감 (그림상 파란색) 

Kv cache 를 많이 가져가서 메모리가 많이 필요함
prefix decoder 는 두가지 컨셉으로 이용가능.
RAG 는 앞에부분이 동일한게 들어가서 처리하거나,
Input prompt 를 굉장히 길게 가져갈 수 있음. (거의 책 한권)
이미지를 집어넣을 수도 있고 (이미지 캡셔널)
앞부분은 그렇게 처리하고, 나머지는 Causal 로 

---------------------------------

Vanilla Acrhitecture Transformer
트랜스포머 아키텍처는 그리 복잡하지 않음.
Q K V (삼지창이라 부름)
인풋이 그냥 복사됨 
인풋X 가 Q; K; V 로 분기되서 그냥 세배로 뻥튀기 되는 것인데. (같지는 않음)
Q에 해당하는 weight, K에 해당하는 , V 에 해당하는 

Q, K 디멘젼의 관계를 파악하고  attention score
입력한 실제 문장 값을 계산해서 결과를.
scaled dot Product Attention (곱하기니까 닷연산)

토크나이저를 통한다음에 dimesion 을 멀티헤드로 쪼개서
QKV 를 헤더만큼 분기를 만들어서 
마지막에 다시 concat 합치는게 멀티 헤드 어텐션

Position붙은건 완벽하게 병렬처리가 가능함.
각각의 위치정보가 굉장히 중요한 영향을 주는데

model parallel 할때 쪼개는건 헤더로 쪼개는건 쉽고
다르게 쪼개는건 도전적인 부분..

----------------------------------------
Attention Mechanism
Attention Visualization
서로간의 relationship 을 학습해서

버트비즈라는 오픈소스 툴이 있는데 가시화 할 수 있음.
현재모델의 각 레이어별로
텍스트가 인풋되면 weight 가 어떻게 나오는지 가시화 가능

------------------
Self Attention Illustation
결국 단순한 행렬 연산에 불과함
Attention (Q, K, V) = softmax(.._)

패딩마스크는 마이너스 무한대로 해서 계산하지 않게끔.

----------------------------------
Types of Attention Mechanism

멀티 쿼리는 모든걸 똑같이 Q,K,V 관점에서 큐가 하나.


라마가 채택한 그룹 쿼리 어텐션
Q, K, V 계산할때 그룹으로 처리

FlashAttention
PagedAttention

어떻게 하면 어텐션을 빨리 계산할까에 대한 그런 것

---------------
최근에는 포지션 
얀? 다양한 기법들
비전 문제에서는 배치 노말라이제이션을

LLM에서는 LayerNorm 을 씀.

그 외에도 
RMSNorem, DeepNorm 등 다양한 놈 기법이 있음.
----------------------------------------------------------
Normalization Position & Activation Function

텍스트를 집어넣으면 
LLM 을 만든걸 가지고 텍스트로 변환해야하는데 (Inverse)
Post-LN 을 해야하는 레이어가있고 Pre-LN을 해야하는 게 있거..

--------------------------------------
position Embeddings
요즘 많이쓰는건 RoPE
학습할때는 4k, 추론할때는 16k 늘려서
Absolute 는 
Relatice 는 포지션 임베딩이 바깥쪽으로 나와있고
RoPE는 포지션 임베딩이 안쪽으로 들어가있음

----------------------------------------
Greedy.. 는 강의에서 생력
LLM 계산할때 temperature, top k 주는데 그런거 설정하게 된 원인이
디코딩할떄 그리드 서치, 빔서치 등에서파생되었음.


-----------------------------
오늘 실습을 위한건
오늘은 Llama 8bilion 을 가지고 실습.
미세조정.
instruction, alignment
최근엔 rainforce도 많이 쓰고.

최근에는 인스트럭션된 모델을 가지고
프롬프트만 바꿔줘서
제로샷, 퓨샷으로 충분히 성능이 가능하기 때문에
파인튜닝 안하는 경우도 많음.
하지만 이뗴ㅒ 문제는 퓨샷을 위한 인스트럭터가 길어져서 (프롬프트가 엄청 커져서..)

------------------------------------------------
Formatted Instance Constuction
CoT 같은걸로 생성한 인스트럭터, 롤, 어시스턴트 등을 만드러주면
각각의 태스크를 잘 만들어주면
템플릿과 데이터셋을 만들 수 있음.
그렇게 데이터셋을 만들고 그 데이터셋으로 학습..

카타스 포갓을 일으키지 않기 위한..
과학계산을 만들기 위한 LLM 을 만들더라도,
프롬프트채팅프롬프트를 넣어서 대화가 잘 ㄷ진행되게..

인스트럭션 튜닝을 원샷에 끝내지않고
스테이지별로 진행해서 (아까 섞어서 하는 그런것)

0----------
요즘 핫한 주제는 신세틱 데이터를 생성하는것.
70 b 모델을 가지고 생성한 텍스트를 가지고 8b 모델을 학습시키거나.

-------------------------------------------------
튜닝기법.. Alignment 
Helpfilness, Honesty, Harmlessness
보통 개인 연구는 정확도만 보면되지만
기업에서는 이런게 중요한.. 
학습 데이터 에서 처리해야할 수도 있고
인스트럭션에서 처리해야할 수도 있고
이걸 거르기 위한 모델을 하나 더 껴놔도 되고

휴먼 피드백데이터가 있으면 정말 좋고..
SFT를 학습하면 참만 학습하는데,
Socre 정보를 주면
이거 안좋으니까 안좋다고 알려줄 수 있고..

reinforce가 좋은이유가 이런 휴먼 피드백
----------------------------------------

PEFT 를 이용하는게 오늘의실습
허깅페이스에서 라이브러리러ㅗ 제공함.
로라, 큐 로라 등 요즘 많아지고있는기법임
프리트레이닝을 가지고, 파라미터를 만들어서,  모델을 여러개 만들어서
여기에다가
로라 기법을 이용하면 어댑터를 붙일 수 있다고 함.
(LLM 보다는 text to image 에서 많이 사용된다고 함)

어답터만 붙이면, 나머지는 프리징 되어있고, 어답터에는 근데 maximum 이 있다고함.
어답터 여러개 쓰기 쉽지않음
어답터 된거에 학습학습 계속하는것도 어려움


------------------
오늘 실습 LoRA
pretrained W가 8빌리언이면 옵티마이저도 무시하지 못함.
A * B 매트릭스로 대체한다??
업데이트된 W가 만들어지면, 로라가 만들어졌으면
Merged W 만 가지고 추론에 사용하는것.
기존 체크포인트에 대해 가중치 사용해도 됨...

로라 스위칭을 할 수 있는 트리톤 서버라던가  텐서 LLM 등이 ..

Quatization 기법이 많이 쓰임.ㄱ
Quantization-aware 나
학습 끝나고 추후에 따로하는 Post training
이미지 비전보다 오히려 LLM 이 ㅋQuantization이 잘 됨

이제까지 말한건
LoRA: Low-Rank Adaptation of LLM 을 위한 것.
우리가 사용할 건 nimos 혹은 허깅페이스 트랜스포머, 

---------------------------------------------------------

한시간동안 이론 공부..

챌린지 두개
오늘 저녁에 해보라고!


ssh ssh.axisapps.io  -l 47a19588435942dfafb8bb83129ef73c

-----------------------------------------------------------
model 폴더
실습에 쓸건 Llama-2-7b-chat

데이터 전처리, 모델 실행하는 코드들은 Llama2 폴더 내에

데이터 다운로드 하는 코드랑..
웨이트를 hf 로 바꾸고 텐서로 바꾸는 코드도 있고..


주피터를 내려받고 실행하려고 하면 에러가 발생할 수 있음.
작년에 만들어둔 코드라서 transformer 쪽이 환경이 달라서.. 이건 세션끝나고 코드 제공해준다고함.

--------------------------------------------------------

Apptainer> pwd
/bootcamp_workspaces/klm-v5xx45

Apptainer> nvidia-smi
Tue May 20 23:40:40 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:87:00.0 Off |                    0 |
| N/A   34C    P0              64W / 400W |      4MiB / 81920MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Apptainer> 


Apptainer> nvidia-smi -l 1 | grep MiB
| N/A   34C    P0              63W / 400W |      4MiB / 81920MiB |      0%      Default |
| N/A   34C    P0              72W / 400W |      4MiB / 81920MiB |      0%      Default |

위는 내 장비의 정보를 1초마다 확인

Apptainer> nvidia-smi -lms 100 | grep MiB
더빠르게1


llama-chat-finetune.ipynb 에서
!ls -LR ../../model/Llama-2-7b-chat
이 명령어 수행해보면 받아져 있는 파일 확인 가능

오리지널 체크포인트를 그대로 받은 파일이라고 함.



Data Preprocessing
openassistant-guanaco 데이터
요즘 많이 쓰이고 있는 포맷이라고 함.
### Human:
### Assistant


데이터 셋 업으면
!python3 ../../source_code/Llama2/download-guanco-ds.py
이거 수행

최근에 NeMo 가 트랜스포머를 채택했다고 함

!python3 ../../source_code/Llama2/download-guanco-ds.py
결과
Downloading...
From: https://drive.google.com/uc?id=1PFLTYBJi0rLQlFjWrr8ioaIgoNbiffXx&confirm=t
To: /workspace/data/openassistant_best_replies_eval.jsonl
100%|██████████████████████████████████████| 1.11M/1.11M [00:00<00:00, 36.3MB/s]
Downloading...
From: https://drive.google.com/uc?id=1tAJI0z_dyZAX9MW6tYmv3znH_561BDVY&confirm=t
To: /workspace/data/openassistant_best_replies_train.jsonl
100%|██████████████████████████████████████| 20.9M/20.9M [00:00<00:00, 52.6MB/s


아래 코드의
TrainingArguments
이게 지금 문제라고함. 버전업이 되어서..
pipy 로 그래서 개인 환경에서 돌릴땐 에러가 날 수 있다고함.

trl 도 1년전과 모델 파라미터들이 좀 변경되었다고 함

----------------
# In some cases where you have access to limited computing resources, you might have to uncomment os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64" if you run into not enough memory issue 


import os
import torch
import json
from datasets import load_dataset, load_from_disk
from langdetect import detect
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
import re
from peft import LoraConfig, PeftModel
from trl import SFTTrainer

#os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

위 임포트를 수행하자 4-> 7 로 변경됨
 37C    P0              85W / 400W |      4MiB / 81920MiB |      0%      Default |
| N/A   37C    P0              84W / 400W |      4MiB / 81920MiB |      0%      Default |
| N/A   37C    P0              85W / 400W |      4MiB / 81920MiB |      0%      Default |
| N/A   37C    P0              84W / 400W |      4MiB / 81920MiB |      0%      Default |
| N/A   37C    P0              85W / 400W |      7MiB / 81920MiB |      0%      Default |
| N/A   37C    P0              85W / 400W |      7MiB / 81920MiB |      0%      Default |


가나코셋은 어느정도 정리가 된 데이터셋
데이터셋마다 포맷이 좀 달라서 전처리가중요하다고함


데이터 다쓰면 오래걸리니까
# extract 5000 samples 
train_samples = raw_train_data[:5000]
5000개 로 해두었다고 함 (실습용)

밑에 함수도 데이터 정제하는 부분
(라이브러리로 전수조사해서 영어만 남기고 제거)

라마는 포맷을 바꾸는 연구가 많이 되어있어서 고대로 가져다 쓰면 된다고 함?

데이터 준비가 끝나면
이제 라마2 학습
허깅페이스통해서 받으려면 api 토큰있어야 함..
FEFT 는 허깅페이스 trl 로 바꿔야 하는데
(메타가 이미 제공중 깃허브에) 


모델 저장할때 위치가.. 내일 텐서RT LLM 으로 할떄 사용하는 주소들이라서 일단 놔두라고..


FEFT 가 레이어 마다마다 디컴포즈 머를? 해주는건데
 RoLa 가 30개 만들어지는 것..
옵션 조절하는건 FEFT 논문에 많이 나와있고, 

그다음은 퀀타이즈 로라.
메모리에 올리는거 설정

FEFT 가 지원하는 비트앤바이트 컨피규를 사용..?

로딩 체크포인트 샤드는 CPu로 올리고.. (그냥 계속 소스 설명임)


trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    eval_dataset = eval_dataset,
    dataset_text_field="text",
    peft_config=peft_params,
    args=training_params,
    max_seq_length=512,
    packing=False,
)


여기까지 트레이닝 준비동작


학습하려고 하는 모델들은 전부 프롬프트의 포맷이 다르기 떄문에
(학습 자체는 인풋 똑같은데  프롬프트가 그렇)

Aron? 은 시스템 롤 없이가고..
싱크를 넣어주기 위해 싱크태그? 를 넣어주거나..
로우 데이터는 그냥 로우 대로 유지하고
데이터를 불러와서 모델 학습할때, 컨버전..?

굉장히 오래걸리는 학습..

강사님은 파라미터가 바뀐 부분에 대해 자료를 공유해주신다고 함..


참고할 깃허브
https://github.com/NVIDIA/NeMo/tree/main/tutorials/llm/llama/nemo2-sft-peft
https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags



로라쪽 그림 이해할것..

NeMo 가 예전에는 트랜스포머따로 네모따로 였는데
점점 통합화??? 되어가고 있따고???

토치 닷 컴파일도 해보라고 하심..

어떤분 Q.




-------------------------------------------------------------------------------
ssh ssh.axisapps.io  -l 838a10a836e64c16839fb9182e0c4781



---------------------------
HPE AI Server Overview
개발자들이 인프라에 대해 좀 알면좋겠다 싶은 부분을 오버뷰

CPU도 빠질 수 없는 부분
intel, 
zeon 6 라는게 나와있음
E-Core, P-Core 
P-Core 라는  CPU 가 GPU 가속 관련해서 지원해줌??
인텔에서는 Intel AMX 라는게 AI 업무를 최대 스물몇배까지 

AMD 5세개
Gen11 서버에서 지원 160코어까지 지원 -> 추후에는 192코어까지 지원될예정

AVX-512 는 가속 기능은 없음

------------------------
Nvidia GPU 현재 지원 GPU 기준
슈퍼칩 형태 HH200
H100  H200 의 성능차이 - 큰 차이가 없음.. H100 NVL이 더 적은 전력을 사용

H200 은 GPU 간의 속도가 빠른게 특징
NVLink NVSwitch?
NVLINK 는 GPU 간의 속도가 어떻게 되느냐
NVSwitch 
이 두개를 지원함으로 인해 8개의 GPU 가 동일하게 빠른 속도를 낼 수 있음.

H200 NVL 은 2way 혹은 4way 로 구성할 수 있음.
GPU간의 속도가 900GB/s


PCIe 타입 을 NVLink 로 묶어서 4way 방식으로



SXM 타입
GPU가 8개가 장착이 되고, NNVLink, NVSwitch 를 통해 

일반적인 pCie 타입은 128GB/s 밖에 안된다고 함.

L40S 도 많이 쓰임. Nvidia ai, 그래픽 범용적인 GPU
A100 보다 살짝 높은 성능

GH200 - CPU GPU 가 하나의 칩으로 구성되어있음.
CPU와 GPU 간의 속도는 원래 빠르지않는데, 이거는 Grace 라는걸 써서 CPU GPU 간의 속도도 높임.

Blackwell 이라는 GPU 도 나옴. 
이것도 CPU GPU가 하나로 있는 칩 - LLM 생성형 AI에서 굉장히 빠른 속도를 낼 수 있는 GPU


Gen AI 인프라 구성의 중요요소


GPU 
CPU
GPU to CPU
Network
Storage


GPU 한장 다 필요없다. 난 반장만 있어도 된다!
하면 하이퍼바이저를 통해서 GPU 가상화를 함
vGPU 가 있으면 
Hypervisor 를 통해, 그 안에 Nvidia vGPU Manager 를 깔아서
vGPU 를 n기가로 쪼개서 인프라 관리자가 개발자들에게 할당함.
CUDA 코어를 할당하는게 아닌, 타임 스래싱
-----------------------------------------------------------------

MIG 는 타임슬래싱 방식이 아니라
물리적으로 완전히 쪼개주는것

Nvidia GPU 가상화 -MIG 기반 쿠버네티스 환경 예시
-------------------------------------------------
nvidia ai enterprise 라이센스 (Full stack sw product)


CUDA{ 와 ADM 의 ROCm은 어떤 차이가 있을까

HPE ProLiant Compute Gen12 서버 라인업


CrayXD
DLC가 요즘 많이 얘기되고 있다고..

Power-War and AI trend  에서 냉각시스템의 중요성이 높아짐
이게 Multi-Core trend 랑

HPE 수냉 서버 지원

수냉식 솔루션
Liquid to Air Cooling
HPE 만 100 % Direct Liquid Cooling 을 지원한다고 함 (그런 서버가 있음/)

AI Mod POD
데이터 센터에서 기대하는 모든 것을 포함하여 모듈형 데이터 센터로 제공

---------------------------------------------------------
Nvidia세션


어제는 Llama 모델을 파인튜닝하는방법
오늘은 추론할 때 학습한 모델을 서빙하는게 목표이기 떄문에,
서빙과 관련된 이론적인 백그라운드 에 대한 설명,
nvidia 가 제공하는 tensorRT 에 대한 설명


-------------------------------
추론이라는건
모델 사이즈가 커지면서 pre training, post training 이중요했다면 요즘엔
inference cycle 이 중요해진 시점
3b, 7b 사이즈가 커질 수록 추론의 속도도 늘어났었는데,
이제는

Post training 은 인스트럭트 튜닝 같은 부분 좀더 우리가 원하는 형태로 맞추는 과정
post training 을 얼마나 효율적으로 학습을 할 수 있는지가 화두였었음. 전엔

최근 이슈는
사람에게 맞게 얼라인된 모델을 얼마나 더 잘 쓸 수 있을까
-> Test-time scaling  Long Thinking

LLM 입장에서는 오래 추론을 한다는 건, 앞선 단어들에 컨디셔널하게
다음 아웃풋을 생성하는 과정인데
인풋 아웃풋이 점점 길어지면서 추론에서점점 많은 시퀀스를 봐야하는
상황이 온다는 것
이제
pre, post 를 넘어 추론을 얼마나 효율적으로 진행할 수 있는지가 
서비스의 성능을 높이며성능을 좌우시키는 것

---------------------------------------------------
TensorRT-LLM Optimizing LLM Inference

엔비디아의 라이센스 필요없이오픈소스로 사용할 수 있는 추론 툴
Nvidia GPU 에 최적화

tensorRT 라는 걸로 시작해서 LLM 이라는 랩퍼가 씌워진거
TensorRT 는 LLM 뿐만 아니라 범용적으로 딥러닝 추론에서 사용되던 라이브러리임.
그래프에 최적화된 아웃풋을 생성할 수 있도록 하는 것.
이미지 프로세싱을 한다던가 의료 이미지를 세그먼트 한다던가
BERT 같은 옛날 모델에도 쓴다던가

KV 캐싱이나 MHA 커널 이런게 LLM을 위해 추가된것이
TensorRT-LLM


Inflight batching
KV Cache
Multi GPU
Multi Node

인플라잇 뱃칭은 GPU 를 얼마나 효율적으로 사용할건가
모델을 학습했으면,
모델을 쿼리 하나하나 넣어서 추론을 하는걸 넘어서
좀도 스케일러블하게.
유저의 인풋이 배치단위로 넘어오고
(배치단위로 받는 이유는, 모델을 서빙했을때, 모델이 사용중인 GPU가 30퍼라면
나머지 메모리 70퍼센트를 사용하기 위해)

static 한 배칭
시퀀싱 단위로 미리 정해주는게 아니라
이터레이션 을 할때 정해주는 거라고 함..
새로운 입력은 in fill..

KV Cache
Q K V
유저가 인풋을 집어넣으면
이 인풋은 결코 변하지 않는, 문장.
어텐션은 새로운 인풋에 대한 쿼리가 전체 키랑 밸류에 대해
유사도를 예측하는 메커니즘

이 전체의 k, v 는 사실상 고정
앞선 인풋들에 대해 k,v 를 재계산할 필요 없게 저장했다가 불러오고,
~~은 다음연산에 추가해서 계산하게..?

-----------------
Paged KV Cache
모델 연산이 효율적이긴 함
trade off 는 캐시를 해야하니까 메모리를 잡아먹을 수 밖에 없음
문장이길어질수록 점점 연속된 공간을 할당해야 하므로 많이 잡아먹는.

(이게 기존의 레거시 코드라고함)
이 메모리 할당을 좀 어떻게 할 수 없을까 에서 시작된게 Paged KV Cache

물리적으로 연속된 공간에 있찌 않아도,
논리적으로 연속된 공간에 있다면, 쓸 수 있지않을까 하는.

TensorRT-LLM
엔진을 먼저 빌드
- 어제 허깅페이스로 모델을 체크포인트를완성했다면,
- 니모 포맷이든 파이토치는 잭스든, 
- 텐서 TR 형태로 변환.
(변환된 것들이 TensorRT 의 체크포인트 에 안에 들어있을 거)

빌드를 하면 특정 태스크에서 사용할 수 있도록 
그래프를 읽어서, 최적화할 수 있는 포인트들이 뭔지 본 후
(매번 빌드를 해야하는 불편함은 있지만, 커널레벨 최적화가 된다면 장점)

워크로드의 속도를 빠르게 하는데 최적화되어있는..

그다음실행..

오늘 세션에서는 LLama 를 기준으로
TensorRT 이 어떤 기능들을 지원하는지 알아볼 것
왠만한 양자화 메소드는 다 지원이 된다고 함.

vLLM 보다 에이백? H100에서는 더 성능이 좋아진다

fp16에서 int8ㅜ로가면 한 네배는 빨리진다고함

00000000000000000000000000
텐서 RT 를 서빙하고 추론하는 방법 
중 전통적인 방법에 대해 오늘 실습

만들어진 걸 돌리려면 백엔드가 필요할텐데,
그거 엔비디아 깃헙에서 제공


TRT-LLM Engine + Triton TRT-LLM Backend
이 필요하고
Model REpository 를 만들고, 이걸기준으로
서버 클라이언트를 런치해서

앞서 빌드했던, 엔진이들어있는 공간(주피터랩에서)
TensorRT-LLM

트랜스포머 코어와 관련된 모델들
앞뒤 단의 병렬적이지 않게 처리되는 파이썬 단.
pre, post processing 은 파이썬레벨

추론 엔진인 텐서알티단

이걸 사용해서 추론하는게 트리톤 인퍼런스 서버

pre processing - TensorRT-LLM - Post processing - Ensemble
최종적으로 앙상블한테 넘겨서, 전체 시스템을 관장하게

-------------------------------------------------------------------

 ssh ssh.axisapps.io  -l 3a49d664b8c94ab8812522ba5a235c44


학습된 체크포인트들을 TRT 형태로 바꿔야 한다.

허깅페이스에있는 config 를 뽑아서 arg 에 저장해줌

kpt 는 피크를 사용해서 하는데..
이건 보안적으로 더 안전하고.?
최근에는 거의 이 safetensor 기반으로 저장한다고 함
weight.safetensor
