##### 시험 요약 공부

### Overfitting
    과적합 (train 에 대해서는 잘 하지만 Test 에 대해서는 못하는 일반성이 떨어지는 것)
    원인은 데이터셋이 너무 적거나, 모델이 너무 복잡한 경우


### 앙상블 학습의 대표적인 NonLinear model (비선형 모델)
#### Random Forest 
     sample 도 feature 도 랜덤하게 뽑아서 트리를 만들고 .. 
    (데이터 샘플과 특성을 일부씩 랜덤으로 뽑고, 결정 트리 모델을 여러개 사용하는 것)

    랜덤 포레스트 란 여러개의 Decision Trees (결정 트리) 를 사용하는 앙상블 학습 기법이다.
    여러개의 Tree 를 독립적으로 학습시키고, 각 트리의 예측 결과를 투표 또는 평균내어 최종 예측을 만드는 것을 아이디어로 하고 있다.
    랜덤 포레스트는 분류와 회귀 문제 모두에 쓰인다.
    
    랜덤 포레스트 특징:
    부트스트랩 샘플링 - 각 트리를 학습 할 때, 전체 데이터에서 중복 허용 샘플링을 통해, 데이터의 일부분만 사용한다.
    랜덤성 - 트리 분할 시 무작위로 feature(특징)를 선택하고, 그 중 최적의 특징을 선택해 트리를 구성하여, Tree 간의 상관성을 낮춘다.
    다양성 - 여러 트리의 예측 결과를 조합하므로, Overfitting 에 대한 저항력이 강하다.
    병렬화 가능 - 각 Tree 가 독립적으로 학습되므로, 병렬 처리가 가능하다.

    예를 들면, 
    분류 문제에서는 가장 많은 트리가 예측한 class 를 최종 예측 값으로 선택하거나,
    회귀 문제에서는 트리들의 예측값 평균을 내어 최종 결과로 사용하는 것이다. 

#### Gradient Boosting (그래디언트 B) 
    그래디언트 부스팅이란, 순차적으로 약한 학습기 (일반적으로 결정 트리)를 학습시켜 나가는 앙상블 기법을 말한다.
    첫번째 학습기가 만든 예측 오류(잔차, residuals)를 다음 학습기가 수정해나가며,
    이전 단계에서 발생한 오류를 최소화 하는 방향으로 학습하여, 학습 할 수록 오류가 줄어드는
    구조가 기본 아이디어이다.

    그래디언트 부스팅 특징:
    순차적 학습 
    오류 수정 - 이전 학습기가 예측하지 못한 오류(잔차)를 학습하여 성능을 개선한다.
    과적합 가능성 - 과적합의 위험이 있다. 이 때문에 learning rate(학습률)과 Tree의 depth 등 하이퍼파라미터 조정이 중요하다.
    효율성 - 순차 학습으로, 병렬 처리가 어려울 수 없지만 트리의 개수, 깊이를 잘 조정하면 효율적인 예측 모델을 만들 수 있다.


    >  Q. 거의 경사하강법과 비슷하지 않나..? 
    >  A. 경사하강법은 '최적화 알고리즘', 딥러닝과 선형 회귀에서 weight(모델의 가중치)를 업데이트하며 손실 함수를 최소화 하는 방향으로 이동하는 방식
    >  A. Gradient Boosting 은 앙상블 학습 기법, 결정 트리와 결합해 사용하며 각 트리 학습 시 경사하강법의 아이디어를 사용한 방식.
    


### 분류 모델에서 주로 사용하는 손실 함수
#### Cross Entropy

### F1-score

$$
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
$$

#### 정밀도(Precision):
$Precision = \frac{TP}{TP + FP}$

#### 재현율(Recall):
$Recall = \frac{TP}{TP + FN}$


### Mini-batch 기법
    데이터가 엄청 크면 시간이 너무 많이 걸린다. (딥러닝)
    그래서 한번 할 때 마다 전체 데이터를 쓰지 않고, 조각조각 나눠서 쓴다. (batch)

    이 때 미니배치 경사하강법을 사용한다.
    Mini-batch Gradient Descent:
    전체 데이터를 작은 batch 로 나눠서, '각각의' 배치에서 기울기를 계산하고 가중치를 업데이트 하는 방식
    여러번의 작은 업데이트를 통해 손실을 빠르게 줄일 수 있다는 장점이 있다!
    

#### Stocastic Gradient Descent (SGD, 확률적 경사 하강법)
    하나의 샘플만을 사용해서 기울기를 계산하고 가중치를 업데이트하는 방법.
    학습 속도가 빠르다는 장점이 있으나, 최적의 최소값에 도달하지 못하고 진동 할 가능성이 있다.


##### TODO 아래는 마저 정리해야함...
```


w = w - Loss 데이터가 엄청 크면 시간이 너무 많이 걸림. (딥러닝)



SGD : stocastic Gradient Descent (Batch 임) -> 모든 batch 에 대해 1번 학습을 완료하는 것이 1 epoch



모델을 최적화 (Optimize) 한다는 것:

Loss 가 최소가 되는 지점을 찾아가는 것



모델의 최적화를 위해 손실 함수를 미분값을 이용해 반대방향을 이용해서 옮겨가며 최저점을 찾는 방법 : 경사하강법



K Fold Cross Validation :

vali 데이터가 충분하지 않을 때에는 block 을 나누고 돌아가면서 훈련시키고 검증을 한다.

cross val score 라는 function 이 해 줌.



Standard Scale ? 표준정규분포화

Standard Scaler 라는 model 을 이용

어떤 값에 대해 평균을 뺀 다음 sigma 로 나누고 ..



쿠퍼스 - 말뭉치



비선형 모델 중 무언가를 prediction 할 때 classification 을 하고 싶을때,

어떤 분류 안에 들어있는 개수

주어진 sample 의 특성 공간에서 거리가 가장 가까운 k 개를 보고, 그것의 평균 치를 보고

샘플이 속할 class 를 예측하는 알고리즘 -> KNN 알고리즘



Decesion Tree 모델

데이터가 모여 있을 때, 어떤 feature 를 기준으로 두개를 나누는데, 그 기준이 되는 것이

현재 상태 복잡도, 불순도 (gini) 를 계산 후, weigthed gini 가 얼마나 줄어들 건지를 보고..

불순도가 가장 낮아지는 방향으로 바꾼다..



gini 수식, 계산 방법 알기 => gini = 1 - P^2...



Loss 가 점점 줄어야 하는데 다시 올라가는 경우..

그 적절한 지점을 찾아야 하는 데

Tree 의 Max Depth 를 지정해주거나..



서포트 벡터 머신과

Linear 모델의 차이점 알기

```

