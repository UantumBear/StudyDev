DeepSeek-R1: Incentivizing Reasoning Capability in Large Language Models

### 초록

##### 딥시크R1-zero
Supervised Fine-Tunning 없이 Reinforcement(강화학습)을 통해서 학습한 모델.
가독성 저하와 언어 혼합 문제가 있었다.

##### 딥시크R1
위 문제를 해결하기 위해, 강화학습을 하기 전에, Multi-stage Training (다단계 학습) 과
Cold-start Data (초기 데이터) 를 활용하였다.
결과적으로 OpenAI o1-1217 과 유사한 성능을 달성.
그리고 연구 지원을 위해
distillation(압축)된 모델 1.5B, 7B, 8B, 14B, 32B, 70B 를 오픈소스로 공개했다.
이 모델들은 Qwen 과 Llama 를 기반으로 제작되었다

### 서론
Post-Training (후속학습) 은 현재 전체 학습 파이프라인에서 중요한 요소로 자리잡았다.
후속학습은 추론의 정확도를 높이고 사용자 선호도에 맞게 모델을 조정하는데 기여한다.
또한, Pre-Training (사전학습)에 비해 상대적으로 적은 자원이 들어간다.
